{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distribution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3_HR2r0oK-Am",
        "F-q9ec13VV5z",
        "BMatkr4R_iWQ",
        "JNiOich9viuc",
        "0SV9TcHxDotZ",
        "-b245eNov4b1",
        "HDjiGUkN7g5e",
        "B-_X32rvD9up",
        "1VOWD2aPGT43"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHP6quamKpem"
      },
      "source": [
        "# INITIALISATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-RRW9zgdKv5"
      },
      "source": [
        "from time import time\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import to_pil_image, resize, to_tensor\n",
        "from torchvision.transforms.functional import normalize\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from zipfile import ZipFile, ZIP_DEFLATED\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcpDqZ1gts-9"
      },
      "source": [
        "SEEDS = [0,1,2,3,4]\n",
        "FORCE = False\n",
        "INTENS = 0.4\n",
        "DEFAULTS = {\n",
        "            \"w0\": 0.2,  # float >= 0, regularisation parameter\n",
        "            \"w\": 0.2,   # float >= 0, harmonisation parameter\n",
        "            \"lr_gen\": 0.02,     # float > 0, learning rate of global model\n",
        "            \"lr_node\": 0.02,    # float > 0, learning rate of local models\n",
        "            \"NN\" : \"base\",     # \"base\" or \"conv\", neural network architecture\n",
        "            \"opt\": optim.Adam,    # any torch otpimizer\n",
        "            \"gen_freq\": 1,         # int >= 1, number of global steps for 1 local step\n",
        "\n",
        "            \"nbn\": 1000,    # int >= 1, number of nodes\n",
        "            \"nbd\": 60_000,  # int >= 1, nbd/nbn must be in [1, 60_000], total data\n",
        "            \"fracdish\": 0,   # float in [0,1]\n",
        "            \"typ_dish\": \"zeros\",# in [\"honest\", \"zeros\", \"jokers\", \"one_evil\", \n",
        "                                   # \"byzantine\", \"trolls\", \"strats\"]\n",
        "            \"heter\": 0,        # int >= 0, heterogeneity of data repartition\n",
        "            \"nb_epochs\": 100 # int >= 1, number of training epochs\n",
        "            }\n",
        "def defaults_help():\n",
        "    ''' Structure of DEFAULTS dictionnary :\n",
        "    \n",
        "        \"w0\": 0.2,  # float >= 0, regularisation parameter\n",
        "        \"w\": 0.2,   # float >= 0, harmonisation parameter\n",
        "        \"lr_gen\": 0.02,     # float > 0, learning rate of global model\n",
        "        \"lr_node\": 0.02,    # float > 0, learning rate of local models\n",
        "        \"NN\" : \"base\",     # \"base\" or \"conv\", neural network architecture\n",
        "        \"opt\": optim.Adam,    # any torch otpimizer\n",
        "        \"gen_freq\": 1,     # int >= 1, number of global steps for \n",
        "                                                        1 local step\n",
        "\n",
        "        \"nbn\": 1000,    # int >= 1, number of nodes\n",
        "        \"nbd\": 60_000,  # int >= 1,  total data\n",
        "                                    - nbd/nbn must be in [1, 60_000]\n",
        "        \"fracdish\": 0,   # float in [0,1]\n",
        "        \"typ_dish\": \"zeros\",# in [\"honest\", \"zeros\", \"jokers\", \"one_evil\", \n",
        "                                \"byzantine\", \"trolls\", \"strats\"]\n",
        "        \"heter\": 0,        # int >= 0, heterogeneity of data repartition\n",
        "        \"nb_epochs\": 100, # int >= 1, number of training epochs\n",
        "    '''\n",
        "    None\n",
        "\n",
        "METRICS = ({\"lab\":\"fit\", \"ord\": \"Training Loss\", \"f_name\": \"loss\"}, \n",
        "           {\"lab\":\"gen\", \"ord\": \"Training Loss\", \"f_name\": \"loss\"}, \n",
        "           {\"lab\":\"reg\", \"ord\": \"Training Loss\", \"f_name\": \"loss\"}, \n",
        "           {\"lab\":\"acc\", \"ord\": \"Accuracy\", \"f_name\": \"acc\"}, \n",
        "           {\"lab\":\"l2_dist\", \"ord\": \"l2 norm\", \"f_name\": \"l2dist\"}, \n",
        "           {\"lab\":\"l2_norm\", \"ord\": \"l2 norm\", \"f_name\": \"l2dist\"}, \n",
        "           {\"lab\":\"grad_sp\", \"ord\": \"Scalar Product\", \"f_name\": \"grad\"}, \n",
        "           {\"lab\":\"grad_norm\", \"ord\": \"Scalar Product\", \"f_name\": \"grad\"}\n",
        "           )\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtHuJPre3AdE"
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "os.makedirs(\"distribution\", exist_ok=True)\n",
        "os.chdir(\"/content/distribution\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Aro-tEK5zL"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3plCM_hmKD2D"
      },
      "source": [
        "## functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChzUjVOMNc3"
      },
      "source": [
        "# data import and management\n",
        "\n",
        "def load_mnist(img_size=32):\n",
        "    ''' return data and labels for train and test mnist dataset '''\n",
        "    #---------------- train data -------------------\n",
        "    mnist_train = datasets.MNIST('data', train=True, download=True)\n",
        "    data_train = mnist_train.data\n",
        "    labels_train = [mnist_train[i][1] for i in range(len(data_train))]\n",
        "\n",
        "    pics = []\n",
        "    for pic in data_train:\n",
        "        pic = to_pil_image(pic)\n",
        "        if img_size != 28:\n",
        "            pic = resize(pic, img_size) # Resize image if needed\n",
        "        pic = to_tensor(pic)            # Tensor conversion normalizes in [0,1]\n",
        "        pics.append(pic)\n",
        "    data_train = torch.stack(pics)\n",
        "\n",
        "    #------------------  test data -----------------------\n",
        "    mnist_test = datasets.MNIST('data', train=False, download=True)\n",
        "    data_test = mnist_test.data\n",
        "    labels_test = [mnist_test[i][1] for i in range(len(data_test))]\n",
        "\n",
        "    pics = []\n",
        "    for pic in data_test:\n",
        "        pic = to_pil_image(pic)\n",
        "        if img_size != 28:\n",
        "            pic = resize(pic, img_size)   # Resize image if needed\n",
        "        pic = to_tensor(pic)             # Tensor conversion normalizes in [0,1]\n",
        "        pics.append(pic)\n",
        "    data_test = torch.stack(pics)\n",
        "\n",
        "    return (data_train,labels_train), (data_test,labels_test)\n",
        "\n",
        "def query(datafull, nb, bias=0, fav=0):\n",
        "    ''' return -nb random samples of -datafull '''\n",
        "    data, labels = datafull\n",
        "    idxs = list(range(len(data)))\n",
        "    l = []\n",
        "    h, w = data[0][0].shape\n",
        "    d = torch.empty(nb, 1, h, w)\n",
        "    if bias == 0:\n",
        "        indexes = random.sample(idxs, nb) # drawing nb random indexes\n",
        "    else :\n",
        "        indexes = []\n",
        "        for i in range(nb):\n",
        "            idx = one_query(labels, idxs, bias, fav)\n",
        "            indexes.append(idx)\n",
        "            idxs.remove(idx) # to draw only once each index max\n",
        "    for k, i in enumerate(indexes): # filling our query\n",
        "        d[k] = data[i]\n",
        "        l.append(labels[i])\n",
        "    return d, l\n",
        "\n",
        "def one_query(labels, idxs, redraws, fav):\n",
        "    ''' labels : list of labels\n",
        "        idxs : list of available indexes\n",
        "        draws an index with a favorite label choice \n",
        "        fav : favorite label\n",
        "        redraws : max nb of random redraws while fav not found\n",
        "    '''\n",
        "    lab = -1 \n",
        "    while lab != fav and redraws >= 0:\n",
        "        idx = idxs[random.randint(0, len(idxs)-1)]\n",
        "        lab = labels[idx]\n",
        "        redraws -= 1\n",
        "    return idx\n",
        "\n",
        "def list_to_longtens(l):\n",
        "    ''' change a list into torch.long tensor '''\n",
        "    tens = torch.empty(len(l), dtype=torch.long)  \n",
        "    for i, lab in enumerate(l):                       \n",
        "        tens[i] = lab\n",
        "    return tens\n",
        "\n",
        "def swap(l, n, m):\n",
        "    ''' swap n and m values in l list '''\n",
        "    return [m if (v==n) else n if (v==m) else v for v in l]\n",
        "\n",
        "\n",
        "def distribute_data_rd(datafull, distrib, fav_lab=(0,0), \n",
        "                       dish=False, dish_lab=0, gpu=True): \n",
        "    '''draw random data on N nodes following distrib\n",
        "        data, labels : raw data and labels\n",
        "        distrib : int list, list of nb of data points for each node\n",
        "        pref_lab : (prefered label, strength of preference (int))\n",
        "        dish : boolean, if nodes are dishonest \n",
        "        dish_lab : 0 to 4, labelisation method\n",
        "\n",
        "        returns : (list of batches of images, list of batches of labels)\n",
        "    '''\n",
        "    global FORCING1\n",
        "    global FORCING2\n",
        "    global FORCE\n",
        "    data, labels = datafull\n",
        "    N = len(distrib)\n",
        "    data_dist = []      # list of len N\n",
        "    labels_dist = []    # list of len N\n",
        "    fav, strength = fav_lab\n",
        "\n",
        "    for n, number in enumerate(distrib): #for each node\n",
        "        # if strength == 0:  # if no preference\n",
        "        d, l = query(datafull, number, strength, fav)\n",
        "        # else:\n",
        "        #     d, l = query(datafull, number, strength, fav)\n",
        "        if gpu:\n",
        "            data_dist.append(torch.FloatTensor(d).cuda())\n",
        "        else:\n",
        "            data_dist.append(torch.FloatTensor(d))\n",
        "        if dish:                # if dishonest node\n",
        "\n",
        "            # labels modification\n",
        "            if dish_lab == 0: # random\n",
        "                tens = torch.randint(10, (number,), dtype=torch.long)\n",
        "            elif dish_lab == 1: # zeros\n",
        "                tens = torch.zeros(number, dtype=torch.long)\n",
        "            elif dish_lab == 2: # swap 1-7\n",
        "                l = swap(l, 1, 7)\n",
        "                tens = list_to_longtens(l)\n",
        "            elif dish_lab == 3: # swap 2 random (maybe same)\n",
        "                if FORCE: # to force same swap multiple times\n",
        "                    if FORCING1 == -1:\n",
        "                        FORCING1, FORCING2 = random.randint(0,9), random.randint(0,9)\n",
        "                    l = swap(l, FORCING1, FORCING2)    \n",
        "                else:         \n",
        "                    n, m = random.randint(0,9), random.randint(0,9)\n",
        "                    l = swap(l, n, m)\n",
        "                tens = list_to_longtens(l)\n",
        "              \n",
        "            elif dish_lab == 4: # label +1\n",
        "                tens = (list_to_longtens(l) + 1) % 10\n",
        "\n",
        "        else:           # if honest node \n",
        "            tens = list_to_longtens(l) # needed for CrossEntropy later\n",
        "        if gpu:\n",
        "            tens = tens.cuda()\n",
        "        labels_dist.append(tens)\n",
        "\n",
        "    return data_dist, labels_dist\n",
        "\n",
        "def zipping(dir_name):\n",
        "    '''zip a local folder to local directory'''\n",
        "    f = ZipFile(dir_name +'.zip', mode='w', compression=ZIP_DEFLATED)\n",
        "    for fil in os.listdir(dir_name):\n",
        "        if fil[0] != \".\":\n",
        "            f.write(dir_name +'/' + fil)\n",
        "    f.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHCoX9RGMtoI"
      },
      "source": [
        "## get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4inHbdwNgz4"
      },
      "source": [
        "# downloading data\n",
        "if 'train' not in globals(): # to avoid loading data every time\n",
        "    train, test = load_mnist()\n",
        "    if torch.cuda.is_available():\n",
        "        test_gpu = torch.tensor(test[0]).cuda(), torch.tensor(test[1]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_HR2r0oK-Am"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QclvVEyDa4G"
      },
      "source": [
        "#model structure\n",
        "\n",
        "def get_base_classifier(gpu=True):\n",
        "    ''' returns linear baseline classifier '''\n",
        "    model = nn.Sequential( \n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1024, 10),\n",
        "        )\n",
        "    if gpu:\n",
        "        return model.cuda()\n",
        "    return model\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    '''CNN Model'''\n",
        "    def __init__(self):\n",
        "        super(classifier, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16,\n",
        "                              kernel_size=3, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, \n",
        "                              kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()      \n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        # Fully connected 1\n",
        "        self.fc1 = nn.Linear(32 * 6 * 6, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.maxpool1(out)  \n",
        "        # Set 2\n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        #Flatten\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #Dense\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "def get_conv_classifier(gpu=True):\n",
        "    if gpu:\n",
        "        return classifier().cuda()\n",
        "    return classifier()\n",
        "\n",
        "MODELS = {\"base\": get_base_classifier, \"conv\": get_conv_classifier}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0v-iNHLBr4"
      },
      "source": [
        "# TRAINING STRUCTURE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-q9ec13VV5z"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PH-0T9rrdr8"
      },
      "source": [
        "#loss and scoring functions \n",
        "\n",
        "def local_loss(model_loc, x, y):  \n",
        "    ''' classification loss '''\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    predicted = model_loc(x)\n",
        "    local = loss(predicted,y)\n",
        "    return local\n",
        "\n",
        "def models_dist(model_loc, model_glob, pow=(1,1)):  \n",
        "    ''' l1 distance between global and local parameter\n",
        "        will be mutliplied by w_n \n",
        "        pow : (internal power, external power)\n",
        "    '''\n",
        "    q, p = pow\n",
        "    dist = sum(((theta - rho)**q).abs().sum() for theta, rho in \n",
        "                  zip(model_loc.parameters(), model_glob.parameters()))**p\n",
        "    return dist\n",
        "\n",
        "def model_norm(model_glob, pow=(2,1)): \n",
        "    ''' l2 squared regularisation of global parameter\n",
        "     will be multiplied by w_0 \n",
        "     pow : (internal power, external power)\n",
        "     '''\n",
        "    q, p = pow\n",
        "    norm = sum((param**q).abs().sum() for param in model_glob.parameters())**p\n",
        "    return norm\n",
        "\n",
        "def round_loss(tens, dec=0): \n",
        "    '''from an input scalar tensor returns rounded integer'''\n",
        "    if type(tens)==int or type(tens)==float:\n",
        "        return round(tens, dec)\n",
        "    else:\n",
        "        return round(tens.item(), dec)\n",
        "\n",
        "def tens_count(tens, val):\n",
        "    ''' counts nb of -val in tensor -tens '''\n",
        "    return len(tens) - round_loss(torch.count_nonzero(tens-val))\n",
        "\n",
        "def score(model, datafull):\n",
        "    ''' returns accuracy provided models, images and GTs '''\n",
        "    out = model(datafull[0])\n",
        "    predictions = torch.max(out, 1)[1]\n",
        "    c=0\n",
        "    for a, b in zip(predictions, datafull[1]):\n",
        "        c += int(a==b)\n",
        "    return c/len(datafull[0])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1BWA7Kk99I"
      },
      "source": [
        "## Flower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMjTiLjZ_nUO"
      },
      "source": [
        "### flower class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CccpUZLozYXm"
      },
      "source": [
        "# nodes repartition\n",
        "\n",
        "class Flower():\n",
        "    ''' Training structure including local models and general one \n",
        "        Allowing to add and remove nodes at will\n",
        "        .pop\n",
        "        .add_nodes\n",
        "        .rem_nodes\n",
        "        .train\n",
        "        .display\n",
        "        .check\n",
        "    '''\n",
        "\n",
        "    def __init__(self, test, gpu=True, **kwargs):\n",
        "        ''' opt : optimizer\n",
        "            test : test data couple (imgs,labels)\n",
        "            w0 : regularisation strength\n",
        "        '''\n",
        "        self.d_test = test\n",
        "        self.w0 = kwargs[\"w0\"]\n",
        "        self.gpu = gpu\n",
        "\n",
        "        self.opt = kwargs[\"opt\"]\n",
        "        self.lr_node = kwargs[\"lr_node\"]\n",
        "        self.lr_gen = kwargs[\"lr_gen\"]\n",
        "        self.gen_freq = kwargs[\"gen_freq\"]  # generalisation frequency (>=1)\n",
        "\n",
        "        self.get_classifier = MODELS[kwargs[\"NN\"]]\n",
        "        self.general_model = self.get_classifier(gpu)\n",
        "        self.init_model = deepcopy(self.general_model)\n",
        "        self.last_grad = None\n",
        "        self.opt_gen = self.opt(self.general_model.parameters(), lr=self.lr_gen)\n",
        "        self.pow_gen = (1,1)  # choice of norms for Licchavi loss \n",
        "        self.pow_reg = (2,1)  # (internal power, external power)\n",
        "        self.data = []\n",
        "        self.labels = [] \n",
        "        self.typ = []\n",
        "        self.models = []\n",
        "        self.weights = []\n",
        "        self.age = []\n",
        "        self.opt_nodes = []\n",
        "        self.nb_nodes = 0\n",
        "        self.dic = {\"honest\" : -1, \"trolls\" : 0, \"zeros\" : 1, \n",
        "                    \"one_evil\" : 2, \"strats\" : 3, \"jokers\" : 4, \"byzantine\" : -1}\n",
        "        self.history = ([], [], [], [], [], [], [], []) \n",
        "        # self.h_legend = (\"fit\", \"gen\", \"reg\", \"acc\", \"l2_dist\", \"l2_norm\", \"grad_sp\", \"grad_norm\")\n",
        "        self.localtest = ([], []) # (which to pick for each node, list of (data,labels) pairs)\n",
        "        self.size = nb_params(self.general_model) / 10_000\n",
        "\n",
        "    # ------------ population methods --------------------\n",
        "    def set_localtest(self, datafull, size, nodes, fav_lab=(0,0), typ=\"honest\"):\n",
        "        ''' create a local data for some nodes\n",
        "            datafull : source data\n",
        "            size : size of test sample\n",
        "            fav_labs : (label, strength)\n",
        "            nodes : list of nodes which use this data           \n",
        "        '''\n",
        "        id = self.dic[typ]\n",
        "        dish = (id != -1) # boolean for dishonesty\n",
        "        dt, lb = distribute_data_rd(datafull, [size], fav_lab,\n",
        "                                    dish, dish_lab=id, gpu=self.gpu)\n",
        "        dtloc = (dt[0], lb[0])\n",
        "        self.localtest[1].append(dtloc)\n",
        "        id = len(self.localtest[1]) - 1\n",
        "        for n in nodes:\n",
        "            self.localtest[0][n] = id\n",
        "\n",
        "    def add_nodes(self, datafull, pop, typ, fav_lab=(0,0), verb=1, **kwargs):\n",
        "        ''' add nodes to the Flower \n",
        "            datafull : data to put on node (sampled from it)\n",
        "            pop : (nb of nodes, size of nodes)\n",
        "            typ : type of nodes (str keywords)\n",
        "            fav_lab : (favorite label, strength)\n",
        "            w : int, weight of new nodes\n",
        "        '''\n",
        "        w = kwargs[\"w\"] # taking global variable if -w not provided\n",
        "        nb, size = pop\n",
        "        id = self.dic[typ]\n",
        "        dish = (id != -1) # boolean for dishonesty\n",
        "        dt, lb = distribute_data_rd(datafull, [size] * nb, fav_lab,\n",
        "                                    dish, dish_lab=id, gpu=self.gpu)\n",
        "        self.data += dt\n",
        "        self.labels += lb\n",
        "        self.typ += [typ] * nb\n",
        "\n",
        "        self.models += [self.get_classifier(self.gpu) for i in range(nb)]\n",
        "        self.weights += [w] * nb\n",
        "        self.age += [0] * nb\n",
        "        for i in range(nb):\n",
        "            self.localtest[0].append(-1)\n",
        "        self.nb_nodes += nb\n",
        "        self.opt_nodes += [self.opt(self.models[n].parameters(), lr=self.lr_node) \n",
        "            for n in range(self.nb_nodes - nb, self.nb_nodes) \n",
        "            ]\n",
        "        if verb:\n",
        "            print(\"Added {} {} nodes of {} data points\".format(nb, typ, size))\n",
        "            print(\"Total number of nodes : {}\".format(self.nb_nodes))\n",
        "\n",
        "    def rem_nodes(self, first, last, verb=1):\n",
        "        ''' remove nodes of indexes -first (included) to -last (excluded) '''\n",
        "        nb = last - first\n",
        "        if last > self.nb_nodes:\n",
        "            print(\"-last is out of range, remove canceled\")\n",
        "        else:\n",
        "            del self.data[first : last]\n",
        "            del self.labels[first : last] \n",
        "            del self.typ[first : last]\n",
        "            del self.models[first : last]\n",
        "            del self.weights[first : last]\n",
        "            del self.age[first : last]\n",
        "            del self.opt_nodes[first : last]\n",
        "            del self.localtest[0][first : last]\n",
        "            self.nb_nodes -= nb\n",
        "            if verb: print(\"Removed {} nodes\".format(nb))\n",
        "        \n",
        "    def hm(self, ty):\n",
        "        ''' count nb of nodes of this type '''\n",
        "        return self.typ.count(ty)\n",
        "    \n",
        "    def pop(self):\n",
        "        ''' return dictionnary of population '''\n",
        "        c = {}\n",
        "        for ty in self.dic.keys():\n",
        "            c[ty] = self.hm(ty)\n",
        "        return c\n",
        "\n",
        "    # ------------- scoring methods -----------\n",
        "    def score_glob(self, datafull): \n",
        "        ''' return accuracy provided images and GTs '''\n",
        "        return score(self.general_model, datafull)\n",
        "    \n",
        "    def test_loc(self, node):\n",
        "        ''' score of node on local test data '''\n",
        "        id_data = self.localtest[0][node]\n",
        "        if id_data == -1:\n",
        "            # print(\"No local test data\")\n",
        "            return None\n",
        "        else:\n",
        "            nodetest = score(self.models[node], self.localtest[1][id_data])\n",
        "            return nodetest\n",
        "\n",
        "    def test_full(self, node):\n",
        "        ''' score of node on global test data '''\n",
        "        return score(self.models[node], self.d_test)\n",
        "\n",
        "    def test_train(self, node):\n",
        "        ''' score of node on its train data '''\n",
        "        return score(self.models[node], (self.data[node], self.labels[node]))\n",
        "\n",
        "    def display(self, node):\n",
        "        ''' display accuracy for selected node\n",
        "            node = -1 for global model\n",
        "        '''\n",
        "        if node == -1: # global model\n",
        "            print(\"global model\")\n",
        "            print(\"accuracy on test data :\", \n",
        "                  self.score_glob(self.d_test))\n",
        "        else: # we asked for a node\n",
        "            loc_train = self.test_train(node)\n",
        "            loc_test = self.test_loc(node)\n",
        "            full_test = self.test_full(node)\n",
        "            print(\"node number :\", node, \", dataset size :\",\n",
        "                len(self.labels[node]), \", type :\", self.typ[node], \n",
        "                \", age :\", self.age[node])\n",
        "            print(\"accuracy on local train data :\", loc_train)\n",
        "            print(\"accuracy on local test data :\", loc_test)\n",
        "            print(\"accuracy on global test data :\", full_test)\n",
        "            repart = {str(k) : tens_count(self.labels[node], k) \n",
        "                for k in range(10)}\n",
        "            print(\"labels repartition :\", repart)\n",
        "    \n",
        "    # ---------- methods for training ------------\n",
        "\n",
        "    def _set_lr(self):\n",
        "        '''set learning rates of optimizers according to Flower setting'''\n",
        "        for n in range(self.nb_nodes):  # updating lr in optimizers\n",
        "            self.opt_nodes[n].param_groups[0]['lr'] = self.lr_node\n",
        "        self.opt_gen.param_groups[0]['lr'] = self.lr_gen\n",
        "\n",
        "    def _zero_opt(self):\n",
        "        '''reset gradients of all models'''\n",
        "        for n in range(self.nb_nodes):\n",
        "            self.opt_nodes[n].zero_grad()      \n",
        "        self.opt_gen.zero_grad()\n",
        "\n",
        "    def _update_hist(self, epoch, test_freq, fit, gen, reg, verb=1):\n",
        "        ''' update history '''\n",
        "        if epoch  % test_freq == 0:   # printing accuracy on test data\n",
        "            acc = self.score_glob(self.d_test)\n",
        "            if verb: print(\"TEST ACCURACY : \", acc)\n",
        "            for i in range(test_freq):\n",
        "                self.history[3].append(acc) \n",
        "        self.history[0].append(round_loss(fit))\n",
        "        self.history[1].append(round_loss(gen))\n",
        "        self.history[2].append(round_loss(reg))\n",
        "\n",
        "        dist = models_dist(self.init_model, self.general_model, pow=(2,0.5)) \n",
        "        norm = model_norm(self.general_model, pow=(2,0.5))\n",
        "        self.history[4].append(round_loss(dist, 1))\n",
        "        self.history[5].append(round_loss(norm, 1))\n",
        "        grad_gen = extract_grad(self.general_model)\n",
        "        if epoch > 1: # no last model for first epoch\n",
        "            scal_grad = sp(self.last_grad, grad_gen)\n",
        "            self.history[6].append(scal_grad)\n",
        "        else:\n",
        "            self.history[6].append(0) # default value for first epoch\n",
        "        self.last_grad = deepcopy(extract_grad(self.general_model)) \n",
        "        grad_norm = sp(grad_gen, grad_gen)  # use sqrt ?\n",
        "        self.history[7].append(grad_norm)\n",
        "\n",
        "    def _old(self, years):\n",
        "        ''' increment age (after training) '''\n",
        "        for i in range(self.nb_nodes):\n",
        "            self.age[i] += years\n",
        "\n",
        "    def _counters(self, c_gen, c_fit):\n",
        "        '''update internal training counters'''\n",
        "        fit_step = (c_fit >= c_gen) \n",
        "        if fit_step:\n",
        "            c_gen += self.gen_freq\n",
        "        else:\n",
        "            c_fit += 1 \n",
        "        return fit_step, c_gen, c_fit\n",
        "\n",
        "    def _do_step(self, fit_step):\n",
        "        '''step for appropriate optimizer(s)'''\n",
        "        if fit_step:       # updating local or global alternatively\n",
        "            for n in range(self.nb_nodes): \n",
        "                self.opt_nodes[n].step()      \n",
        "        else:\n",
        "            self.opt_gen.step()  \n",
        "\n",
        "    def _print_losses(self, tot, fit, gen, reg):\n",
        "        '''print losses'''\n",
        "        print(\"total loss : \", tot) \n",
        "        print(\"fitting : \", round_loss(fit),\n",
        "                ', generalisation : ', round_loss(gen),\n",
        "                ', regularisation : ', round_loss(reg))\n",
        "\n",
        "    # ====================  TRAINING ================== \n",
        "\n",
        "    def train(self, nb_epochs=None, test_freq=1, verb=1):   \n",
        "        '''training loop'''\n",
        "        nb_epochs = EPOCHS if nb_epochs is None else nb_epochs\n",
        "        time_train = time()\n",
        "        self._set_lr()\n",
        "\n",
        "        # initialisation to avoid undefined variables at epoch 1\n",
        "        loss, fit_loss, gen_loss, reg_loss = 0, 0, 0, 0\n",
        "        c_fit, c_gen = 0, 0\n",
        "\n",
        "        fit_scale = 20 / self.nb_nodes\n",
        "        gen_scale = 1 / self.nb_nodes / self.size\n",
        "        reg_scale = self.w0 / self.size\n",
        "\n",
        "        reg_loss = reg_scale * model_norm(self.general_model, self.pow_reg)  \n",
        "\n",
        "        # training loop \n",
        "        nb_steps = self.gen_freq + 1\n",
        "        for epoch in range(1, nb_epochs + 1):\n",
        "            if verb: print(\"\\nepoch {}/{}\".format(epoch, nb_epochs))\n",
        "            time_ep = time()\n",
        "\n",
        "            for step in range(1, nb_steps + 1):\n",
        "                fit_step, c_gen, c_fit = self._counters(c_gen, c_fit)\n",
        "                if verb >= 2: \n",
        "                    txt = \"(fit)\" if fit_step else \"(gen)\" \n",
        "                    print(\"step :\", step, '/', nb_steps, txt)\n",
        "                self._zero_opt() # resetting gradients\n",
        "\n",
        "\n",
        "                #----------------    Licchavi loss  -------------------------\n",
        "                 # only first 2 terms of loss updated\n",
        "                if fit_step:\n",
        "                    fit_loss, gen_loss, diff = 0, 0, 0\n",
        "                    for n in range(self.nb_nodes):   # for each node\n",
        "                        if self.typ[n] == \"byzantine\":\n",
        "                            fit = local_loss(self.models[n], \n",
        "                                             self.data[n], self.labels[n])\n",
        "                            fit_loss -= fit\n",
        "                            diff += 2 * fit # dirty trick CHANGE\n",
        "                        else:\n",
        "                            fit_loss += local_loss(self.models[n], \n",
        "                                                self.data[n], self.labels[n])\n",
        "                        g = models_dist(self.models[n], \n",
        "                                        self.general_model, self.pow_gen)\n",
        "                        gen_loss +=  self.weights[n] * g  # generalisation term\n",
        "                    fit_loss *= fit_scale\n",
        "                    gen_loss *= gen_scale\n",
        "                    loss = fit_loss + gen_loss \n",
        "                          \n",
        "                # only last 2 terms of loss updated \n",
        "                else:        \n",
        "                    gen_loss, reg_loss = 0, 0\n",
        "                    for n in range(self.nb_nodes):   # for each node\n",
        "                        g = models_dist(self.models[n], \n",
        "                                        self.general_model, self.pow_gen)\n",
        "                        gen_loss += self.weights[n] * g  # generalisation term    \n",
        "                    reg_loss = model_norm(self.general_model, self.pow_reg) \n",
        "                    gen_loss *= gen_scale\n",
        "                    reg_loss *= reg_scale\n",
        "                    loss = gen_loss + reg_loss\n",
        "\n",
        "                total_out = round_loss(fit_loss + diff\n",
        "                    + gen_loss + reg_loss)\n",
        "                if verb >= 2:\n",
        "                    self._print_losses(total_out, fit_loss + diff,\n",
        "                                       gen_loss, reg_loss)\n",
        "                # Gradient descent \n",
        "                loss.backward() \n",
        "                self._do_step(fit_step)   \n",
        " \n",
        "            if verb: print(\"epoch time :\", round(time() - time_ep, 2)) \n",
        "            self._update_hist(epoch, test_freq, fit_loss, gen_loss, reg_loss, verb)\n",
        "            self._old(1)  # aging all nodes\n",
        "             \n",
        "        # ----------------- end of training -------------------------------  \n",
        "        for i in range(nb_epochs % test_freq): # to maintain same history length\n",
        "            self.history[3].append(acc)\n",
        "        print(\"training time :\", round(time() - time_train, 2)) \n",
        "        return self.history\n",
        "\n",
        "\n",
        "    # ------------ to check for problems --------------------------\n",
        "    def check(self):\n",
        "        ''' perform some tests on internal parameters adequation '''\n",
        "        # population check\n",
        "        b1 =  (self.nb_nodes == len(self.data) == len(self.labels) \n",
        "            == len(self.typ) == len(self.models) == len(self.opt_nodes) \n",
        "            == len(self.weights) == len(self.age) == len(self.localtest[0]))\n",
        "        # history check\n",
        "        b2 = True\n",
        "        for l in self.history:\n",
        "            b2 = b2 and (len(l) == len(self.history[0]) >= max(self.age))\n",
        "        # local test data check\n",
        "        b3 = (max(self.localtest[0]) + 1 <= len(self.localtest[1]) )\n",
        "        if (b1 and b2 and b3):\n",
        "            print(\"No Problem\")\n",
        "        else:\n",
        "            print(\"OULALA non ça va pas là\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMatkr4R_iWQ"
      },
      "source": [
        "### flower utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqix5WLBWpCp"
      },
      "source": [
        "def get_flower(gpu=True, **kwargs):\n",
        "    '''get a Flower using the appropriate test data (gpu or not)'''\n",
        "    if gpu:\n",
        "        return Flower(test_gpu, gpu=gpu, **kwargs)\n",
        "    else:\n",
        "        return Flower(test, gpu=gpu, **kwargs)\n",
        "\n",
        "# def grad_sp(m1, m2):\n",
        "#     ''' scalar product of gradients of 2 models '''\n",
        "#     s = 0\n",
        "#     for p1, p2 in zip(m1.parameters(), m2.parameters()):\n",
        "#         s += (p1.grad * p2.grad).sum()\n",
        "#     return s\n",
        "\n",
        "def extract_grad(model):\n",
        "    '''return list of gradients of a model'''\n",
        "    l_grad =  [p.grad for p in model.parameters()]\n",
        "    return l_grad\n",
        "\n",
        "def sp(l_grad1, l_grad2):\n",
        "    '''scalar product of 2 lists of gradients'''\n",
        "    s = 0\n",
        "    for g1, g2 in zip(l_grad1, l_grad2):\n",
        "        s += (g1 * g2).sum()\n",
        "    return round_loss(s, 4)\n",
        "\n",
        "def nb_params(model):\n",
        "    '''return number of parameters of a model'''\n",
        "    return sum(p.numel() for p in model.parameters())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNiOich9viuc"
      },
      "source": [
        "# GETTING PLOTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SV9TcHxDotZ"
      },
      "source": [
        "## Plotting utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HdbfF8jDm8R"
      },
      "source": [
        "def seedall(s):\n",
        "    '''seed all sources of randomness'''\n",
        "    reproducible = (s >= 0)\n",
        "    torch.manual_seed(s)\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    torch.backends.cudnn.deterministic = reproducible\n",
        "    torch.backends.cudnn.benchmark     = not reproducible\n",
        "    print(\"\\nSeeded all to\", s)\n",
        "\n",
        "def replace_dir(path):\n",
        "    ''' create or replace directory '''\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n",
        "\n",
        "def get_style():\n",
        "    '''give different line styles for plots'''\n",
        "    l = [\"-\",\"-.\",\":\",\"--\"]\n",
        "    for i in range(10000):\n",
        "        yield l[i % 4]\n",
        "\n",
        "def get_color():\n",
        "    '''give different line styles for plots'''\n",
        "    l = [\"red\",\"green\",\"blue\",\"grey\"]\n",
        "    for i in range(10000):\n",
        "        yield l[i % 4]\n",
        "\n",
        "STYLES = get_style() # generator for looping styles\n",
        "COLORS = get_color()\n",
        "\n",
        "def title_save(title=None, path=None, suff=\".png\"):\n",
        "    ''' add title and save plot '''\n",
        "    if title is not None:   \n",
        "        plt.title(title)\n",
        "    if path is not None:\n",
        "        plt.savefig(path + suff)\n",
        "\n",
        "def legendize(y):\n",
        "    ''' label axis of plt plot '''\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(y)\n",
        "    plt.legend()\n",
        "\n",
        "def clean_dic(dic):\n",
        "    ''' replace some values by more readable ones '''\n",
        "    if \"opt\" in dic.keys():\n",
        "        dic = deepcopy(dic)\n",
        "        op = dic[\"opt\"]\n",
        "        dic[\"opt\"] = \"Adam\" if op == optim.Adam else \"SGD\" if op == optim.SGD else None\n",
        "    return dic\n",
        "\n",
        "def get_title(conf, ppl=4):\n",
        "    ''' converts a dictionnary in str of approriate shape \n",
        "        ppl : parameters per line\n",
        "    '''\n",
        "    title = \"\"\n",
        "    c = 0 # enumerate ?\n",
        "    for key, val in clean_dic(conf).items(): \n",
        "        c += 1\n",
        "        title += \"{}: {}\".format(key,val)\n",
        "        title += \" \\n\" if (c % ppl) == 0 else ', '\n",
        "    return title[:-2]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b245eNov4b1"
      },
      "source": [
        "## Plotting from history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_uPE0aIC0Ai"
      },
      "source": [
        "# functions to display training history \n",
        "\n",
        "def means_bounds(arr):\n",
        "    ''' from array return 1 array of means, \n",
        "        1 of (mean - var), 1 of (mean + var)\n",
        "    '''\n",
        "    means = np.mean(arr, axis=0)\n",
        "    var = np.var(arr, axis = 0) \n",
        "    low, up = means - var, means + var\n",
        "    return means, low, up\n",
        "\n",
        "\n",
        "# ----------- to display multiple accuracy curves on same plot -----------\n",
        "def add_acc_var(arr, label):\n",
        "    ''' from array add curve of accuracy '''\n",
        "    acc = arr[:,3,:]\n",
        "    means, low, up = means_bounds(acc)\n",
        "    epochs = range(1, len(means) + 1)\n",
        "    plt.plot(epochs, means, label=label, linestyle=next(STYLES))\n",
        "    plt.fill_between(epochs, up, low, alpha=0.4)\n",
        "\n",
        "def plot_runs_acc(l_runs, title=None, path=None, **kwargs):\n",
        "    ''' plot several acc_var on one graph '''\n",
        "    arr = np.asarray(l_runs)\n",
        "    l_param = get_possibilities(**kwargs) # for legend\n",
        "    for run, param in zip(arr, l_param): # adding one curve for each parameter combination (run)\n",
        "        add_acc_var(run, param)\n",
        "    plt.ylim([0,1])\n",
        "    plt.grid(True, which='major', linewidth=1, axis='y', alpha=1)\n",
        "    plt.minorticks_on()\n",
        "    plt.grid(True, which='minor', linewidth=0.8, axis='y', alpha=0.8)\n",
        "    legendize(\"Test Accuracy\")\n",
        "    title_save(title, path, suff=\".png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ------------- utility for what follows -------------------------\n",
        "def plot_var(l_hist, l_idx):\n",
        "    ''' add curve of asked indexes of history to the plot '''\n",
        "    arr_hist = np.asarray(l_hist)\n",
        "    epochs = range(1, arr_hist.shape[2] + 1)\n",
        "    for idx in l_idx:\n",
        "        vals = arr_hist[:,idx,:]\n",
        "        vals_m, vals_l, vals_u = means_bounds(vals)\n",
        "        style, color = next(STYLES), next(COLORS)\n",
        "        plt.plot(epochs, vals_m, label=METRICS[idx][\"lab\"], linestyle=style, color=color)\n",
        "        plt.fill_between(epochs, vals_u, vals_l, alpha=INTENS, color=color)\n",
        "\n",
        "def plotfull_var(l_hist, l_idx, title=None, path=None, show=True):\n",
        "    ''' plot metrics asked in -l_idx and save if -path provided '''\n",
        "    plot_var(l_hist, l_idx)\n",
        "    idx = l_idx[0]\n",
        "    legendize(METRICS[idx][\"ord\"])\n",
        "    title_save(title, path, suff=\" {}.png\".format(METRICS[idx][\"f_name\"]))\n",
        "    if show: \n",
        "        plt.show()\n",
        "\n",
        "# ------- groups of metrics on a same plot -----------\n",
        "def loss_var(l_hist, title=None, path=None):\n",
        "    ''' plot losses with variance from a list of historys '''\n",
        "    plotfull_var(l_hist, [0,1,2], title, path)\n",
        "\n",
        "def acc_var(l_hist, title=None, path=None):\n",
        "    ''' plot accuracy with variance from a list of historys '''\n",
        "    plt.ylim([0,1])\n",
        "    plt.grid(True, which='major', linewidth=1, axis='y', alpha=1)\n",
        "    plt.minorticks_on()\n",
        "    plt.grid(True, which='minor', linewidth=0.8, axis='y', alpha=0.8)\n",
        "    plotfull_var(l_hist, [3], title, path)\n",
        "\n",
        "def l2_var(l_hist, title=None, path=None):\n",
        "    '''plot l2 norm of gen model from a list of historys'''\n",
        "    plotfull_var(l_hist, [4,5], title, path)\n",
        "\n",
        "def gradsp_var(l_hist, title=None, path=None):\n",
        "    ''' plot scalar product of gradients between 2 consecutive epochs\n",
        "        from a list of historys\n",
        "    '''\n",
        "    plotfull_var(l_hist, [6,7], title, path)\n",
        "\n",
        "# plotting all we have\n",
        "def plot_metrics(l_hist, title=None, path=None):\n",
        "    '''plot and save the different metrics from list of historys'''\n",
        "    acc_var(l_hist, title, path)  \n",
        "    loss_var(l_hist, title, path)\n",
        "    l2_var(l_hist, title, path)\n",
        "    gradsp_var(l_hist, title, path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rAFZQTjqhDc"
      },
      "source": [
        "## Running, plotting, saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjiGUkN7g5e"
      },
      "source": [
        "### utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Ch1JwHBSfm"
      },
      "source": [
        "def adapt(obj):\n",
        "    ''' -obj is a parameter or an iterable over values of a parameter\n",
        "        return generator of values of the parameter (event if only 1)\n",
        "    '''\n",
        "    if hasattr(obj, '__iter__') and type(obj) != str:\n",
        "        for v in obj:\n",
        "            yield v\n",
        "    else:\n",
        "        yield obj\n",
        "\n",
        "def is_end(it, dist=0):\n",
        "    ''' check if iterator is empty '''\n",
        "    it2 = deepcopy(it)\n",
        "    try:\n",
        "        for a in range(dist + 1):\n",
        "            a = next(it2)\n",
        "        return False\n",
        "    except StopIteration:\n",
        "        return True\n",
        "\n",
        "def explore(dic):\n",
        "    ''' dic is a dictionnary of parameters (some may have multiple values)\n",
        "        return a list of dictionnarys of all possible combinations \n",
        "    '''\n",
        "    it = iter(dic)\n",
        "    _LIST = [] \n",
        "    def _explo(it, dic, **kwargs): # **kwargs is the output\n",
        "        '''yield a dictionnary with only one value for each param'''\n",
        "        if not is_end(it): # if iterator not empty\n",
        "            key = next(it)\n",
        "            for par in adapt(dic[key]):\n",
        "                _explo(deepcopy(it), dic, **kwargs, **{key: par}) \n",
        "        else:            # end of recursion\n",
        "            _LIST.append(kwargs)\n",
        "    _explo(it, dic)\n",
        "    return _LIST\n",
        "\n",
        "def add_defaults(config):\n",
        "    ''' add default values for non-specified parameters '''\n",
        "    fullconf = deepcopy(DEFAULTS)        \n",
        "    for key, val in config.items():\n",
        "        fullconf[key] = val\n",
        "    return fullconf\n",
        "\n",
        "def my_confs(**kwargs):\n",
        "    ''' return all possible configurations '''\n",
        "    for config in explore(kwargs):\n",
        "        fullconf = add_defaults(config)\n",
        "        yield fullconf\n",
        "\n",
        "# FUSE THE 2 FUNCTIONS ?\n",
        "def get_possibilities(**kwargs):\n",
        "    ''' identify variations of parameters '''\n",
        "    l_confs = explore(kwargs)\n",
        "    leg_keys = []  # parameters used for legend\n",
        "    for key, val in kwargs.items():\n",
        "        if len(list(adapt(val))) >  1: # if this param is not constant\n",
        "            leg_keys.append(key)\n",
        "    legends = []\n",
        "    for conf in l_confs:\n",
        "        leg = get_title({k:conf[k] for k in leg_keys})\n",
        "        legends.append(leg)\n",
        "    return legends\n",
        "\n",
        "def get_constants(**kwargs):\n",
        "    ''' identify constant parameters '''\n",
        "    l_confs = my_confs(**kwargs)\n",
        "    leg_keys = []  # parameters used for legend\n",
        "    for key, val in kwargs.items():\n",
        "        if len(list(adapt(val))) >  1: # if this param is not constant\n",
        "            leg_keys.append(key)\n",
        "    constants = []\n",
        "    for conf in l_confs:\n",
        "        cst = get_title({k:conf[k] for k in DEFAULTS.keys() if (k not in leg_keys)})\n",
        "        constants.append(cst)\n",
        "    return constants   # NOT CLEAN BEACAUSE CONSTANT LIST\n",
        "\n",
        "def legend_to_name(legend):\n",
        "    ''' convert legend text format to filename format '''\n",
        "    name = legend.replace(': ','_')       # deepcopy ?\n",
        "    name = name.replace('\\n', ' ')\n",
        "    name = name.replace(',', '')\n",
        "    return name"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2znZ9pVW7lPL"
      },
      "source": [
        "### core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ldygG1rQz4e"
      },
      "source": [
        "def get_custom_flower(verb=1, gpu=True, **kwargs):\n",
        "    nbn = kwargs[\"nbn\"]\n",
        "    ppn = kwargs[\"nbd\"] // nbn # points per node\n",
        "    nbdish = int(kwargs[\"fracdish\"] * nbn)\n",
        "    nbh = nbn - nbdish\n",
        "    typ_dish = kwargs[\"typ_dish\"]\n",
        "    heter = kwargs[\"heter\"]    \n",
        "    flow = get_flower(gpu=gpu, **kwargs)\n",
        "    if heter:\n",
        "        nbh_lab = nbh // 10 # for each label\n",
        "        nbdish_lab = nbdish // 10\n",
        "        nb_lab = nbh_lab + nbdish_lab\n",
        "        for lab in range(10): # for each label\n",
        "            flow.add_nodes(train, (nbh_lab, ppn), \"honest\", (lab, heter), verb=verb, **kwargs)\n",
        "            flow.add_nodes(train, (nbdish_lab, ppn), typ_dish, (lab, heter), verb=verb, **kwargs)\n",
        "            # if gpu:\n",
        "            #     flow.set_localtest(test_gpu, 100, range(lab * nb_lab, (lab + 1) * nb_lab), (lab, heter))\n",
        "            # else:\n",
        "            #     flow.set_localtest(test, 100, range(lab * nb_lab, (lab + 1) * nb_lab), (lab, heter))\n",
        "    else:\n",
        "        # print(kwargs)\n",
        "        flow.add_nodes(train, (nbh, ppn), \"honest\", verb=verb, **kwargs)\n",
        "        flow.add_nodes(train, (nbdish, ppn), typ_dish, verb=verb, **kwargs)\n",
        "    return flow\n",
        "\n",
        "def run_whatever(config, path, verb=0, gpu=True):\n",
        "    '''config is a dictionnary with all parameters'''\n",
        "    nb_epochs = config[\"nb_epochs\"]\n",
        "    l_hist = [] # list of historys\n",
        "    for s in SEEDS:\n",
        "        seedall(s)\n",
        "        flow = get_custom_flower(verb=verb, gpu=gpu, **config) \n",
        "        h = flow.train(nb_epochs, verb=verb)\n",
        "        l_hist.append(h)\n",
        "    title = get_title(config)\n",
        "    plot_metrics(l_hist, title, path)\n",
        "    return l_hist\n",
        "\n",
        "def run_whatever_mult(name=\"name\", verb=0, gpu=True, **kwargs):\n",
        "    ''' User-friendly running-and-plotting-and-saving interface\n",
        "        Each parameter of DEFAULTS can be \n",
        "        inputted as single value, as an iterable of values or not inputted\n",
        "        All parameters combinations are computed in a grid fashion \n",
        "\n",
        "        name : used for folder name and filenames\n",
        "        verb : 0, 1 or 2, verbosity level\n",
        "        gpu : boolean\n",
        "        **kwargs : structure and training parameters, \n",
        "                    see \"defaults_help?\" for full parameters list\n",
        "        Return : all training historys\n",
        "    '''\n",
        "    l_runs = [] # list of historys for each parameter\n",
        "    replace_dir(name)\n",
        "    path = name + \"/\" + name + \" \"\n",
        "    l_legend = get_possibilities(**kwargs)\n",
        "    l_confs = my_confs(**kwargs)\n",
        "    for legend, config in zip(l_legend, l_confs): # iterating over all combinations\n",
        "        curr_path = path + legend_to_name(legend)\n",
        "        l_hist = run_whatever(config, curr_path, verb, gpu)\n",
        "        l_runs.append(l_hist)\n",
        "    title = get_constants(**kwargs)[0]\n",
        "    plot_runs_acc(l_runs, title, path, **kwargs)\n",
        "    zipping(name)\n",
        "    return l_runs"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-_X32rvD9up"
      },
      "source": [
        "## Some more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkv4sxEP3X0j"
      },
      "source": [
        "# functions to train and display history at the end\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# - heterogeneity of data with different styles of notation depending on nodes -\n",
        "\n",
        "# def get_flower_heter_strats(heter, verb=0, gpu=True):\n",
        "#     '''initialize and add nodes according to parameter'''\n",
        "#     global FORCING1\n",
        "#     global FORCING2\n",
        "#     global FORCE    \n",
        "#     nbn = NBN\n",
        "#     flow = get_flower(gpu)\n",
        "#     ppn = 60_000 // nbn # points per node\n",
        "#     nb_lab = nbn // 10\n",
        "#     FORCE = True\n",
        "#     for lab in range(10):\n",
        "#         for n in range(nb_lab):\n",
        "#             FORCING1, FORCING2 = -1, -1\n",
        "#             flow.add_nodes(train, (1, ppn), \"strats\", (lab, heter), verb=verb)\n",
        "#             flow.set_localtest(test_gpu, 100, [lab * nb_lab + n], (lab, heter), typ=\"strats\")\n",
        "#     FORCE = False\n",
        "#     return flow\n",
        "\n",
        "# def run_heter_strats(heter, verb=0, gpu=True):\n",
        "#     ''' create a flower of honest nodes and trains it for 200 eps\n",
        "#         display graphs of loss and accuracy \n",
        "#         heter : heterogeneity of data\n",
        "#     '''\n",
        "#     flow = get_flower_heter_strats(heter, verb, gpu)\n",
        "#     flow.gen_freq = 1\n",
        "#     h = flow.train(epochs, verb=verb)\n",
        "#     flow.check()\n",
        "#     t1 = \"heter : {}, nbn : {}, lrnode : {}, lrgen : {}, genfrq : {}\" \n",
        "#     t2 = \"\\ntype : only strats\" \n",
        "#     text = t1 + t2\n",
        "#     title = text.format(heter, flow.nb_nodes, flow.lr_node, \n",
        "#                         flow.lr_gen, flow.gen_freq)\n",
        "#     plot_metrics([h], title, path)    \n",
        "#     return flow\n",
        "\n",
        "# def compare(flow_centr, flow_distr): # for run_heter\n",
        "#     ''' return average accuracy on local test sets \n",
        "#         for both centralized and distributed models\n",
        "#     '''\n",
        "#     central, gen, distr = 0, 0, 0\n",
        "#     N = flow_distr.nb_nodes\n",
        "#     for lab in range(10):\n",
        "#         sc = score(flow_centr.models[0], flow_distr.localtest[1][lab])\n",
        "#         central += sc\n",
        "#     for lab in range(10):\n",
        "#         sc = score(flow_distr.general_model, flow_distr.localtest[1][lab])\n",
        "#         gen += sc\n",
        "#     for n in range(N):\n",
        "#         sc = flow_distr.test_loc(n)\n",
        "#         distr += sc\n",
        "#     distr = distr / N\n",
        "#     central = central / 10\n",
        "#     gen = gen / 10\n",
        "#     return central, gen, distr\n",
        "\n",
        "#  def compare2(flow_centr, flow_distr): # for run_heter_strats\n",
        "#     ''' return average accuracy on local test sets \n",
        "#         for both centralized and distributed models\n",
        "#     '''\n",
        "#     central, gen, distr = 0, 0, 0\n",
        "#     N = flow_distr.nb_nodes\n",
        "#     for n in range(N):\n",
        "#         sc = score(flow_centr.models[0], flow_distr.localtest[1][n])\n",
        "#         central += sc\n",
        "#     for n in range(N):\n",
        "#         sc = score(flow_distr.general_model, flow_distr.localtest[1][n])\n",
        "#         gen += sc\n",
        "#     for n in range(N):\n",
        "#         sc = flow_distr.test_loc(n)\n",
        "#         distr += sc\n",
        "#     distr = distr / N\n",
        "#     central = central / N\n",
        "#     gen = gen / N\n",
        "#     return central, gen, distr "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oy-l4j-E-Qx"
      },
      "source": [
        "# THAT'S WHERE YOU RUN STUFF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtXXF7pxHkN5"
      },
      "source": [
        "## Help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6iHuy8HFDnR",
        "outputId": "1f7281e7-bc89-4c25-fbea-12a7de15e1fe"
      },
      "source": [
        "help(run_whatever_mult)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function run_whatever_mult in module __main__:\n",
            "\n",
            "run_whatever_mult(name='name', verb=0, gpu=True, **kwargs)\n",
            "    User-friendly running-and-plotting-and-saving interface\n",
            "    Each parameter of DEFAULTS can be \n",
            "    inputted as single value, as an iterable of values or not inputted\n",
            "    All parameters combinations are computed in a grid fashion \n",
            "    \n",
            "    name : used for folder name and filenames\n",
            "    verb : 0, 1 or 2, verbosity level\n",
            "    gpu : boolean\n",
            "    **kwargs : structure and training parameters, \n",
            "                see \"defaults_help?\" for full parameters list\n",
            "    Return : all training historys\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXIY6P3qFGgB",
        "outputId": "2e6cadea-5f03-4009-d098-0eded076417d"
      },
      "source": [
        "help(defaults_help)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function defaults_help in module __main__:\n",
            "\n",
            "defaults_help()\n",
            "    Structure of DEFAULTS dictionnary :\n",
            "    \n",
            "    \"w0\": 0.2,  # float >= 0, regularisation parameter\n",
            "    \"w\": 0.2,   # float >= 0, harmonisation parameter\n",
            "    \"lr_gen\": 0.02,     # float > 0, learning rate of global model\n",
            "    \"lr_node\": 0.02,    # float > 0, learning rate of local models\n",
            "    \"NN\" : \"base\",     # \"base\" or \"conv\", neural network architecture\n",
            "    \"opt\": optim.Adam,    # any torch otpimizer\n",
            "    \"gen_freq\": 1,     # int >= 1, number of global steps for \n",
            "                                                    1 local step\n",
            "    \n",
            "    \"nbn\": 1000,    # int >= 1, number of nodes\n",
            "    \"nbd\": 60_000,  # int >= 1,  total data\n",
            "                                - nbd/nbn must be in [1, 60_000]\n",
            "    \"fracdish\": 0,   # float in [0,1]\n",
            "    \"typ_dish\": \"zeros\",# in [\"honest\", \"zeros\", \"jokers\", \"one_evil\", \n",
            "                            \"byzantine\", \"randoms\", \"trolls\", \"strats\"]\n",
            "    \"heter\": 0,        # int >= 0, heterogeneity of data repartition\n",
            "    \"nb_epochs\": 100, # int >= 1, number of training epochs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAxYhySHrFp"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjG9ImlLHhrB"
      },
      "source": [
        "SEEDS = [1,2,3,4,5]\n",
        "historys = run_whatever_mult(nb_epochs=10, verb=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VOWD2aPGT43"
      },
      "source": [
        "# MANUAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTX8hvSTTWIY"
      },
      "source": [
        "seedall(51)\n",
        "tulip = get_flower(**DEFAULTS)\n",
        "tulip.add_nodes(train, (1, 60000), \"honest\")\n",
        "tulip.check()\n",
        "# tulip.lr_node = 0.2\n",
        "# tulip.lr_gen = 0.05\n",
        "# tulip.w0 = 0\n",
        "h1 = tulip.train(2, verb=2)\n",
        "tulip.check()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYNOaYUHkjQ2"
      },
      "source": [
        "tulip.display(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCC1Q8wlYPkM"
      },
      "source": [
        "plot_metrics([h1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}