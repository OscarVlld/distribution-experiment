{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distribution_mixte.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oHP6quamKpem",
        "7_Aro-tEK5zL",
        "3_HR2r0oK-Am",
        "F-q9ec13VV5z",
        "cC1BWA7Kk99I",
        "AHCoX9RGMtoI",
        "rIv7X1MIW8Qw",
        "vuSRqfKLyWVx"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHP6quamKpem"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-RRW9zgdKv5"
      },
      "source": [
        "from time import time\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import to_pil_image, resize, to_tensor\n",
        "from torchvision.transforms.functional import normalize\n",
        "# from tqdm.notebook import tqdm\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Aro-tEK5zL"
      },
      "source": [
        "## Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChzUjVOMNc3"
      },
      "source": [
        "# data import and management\n",
        "\n",
        "def load_mnist(img_size=32):\n",
        "    ''' returns data and labels for train and test mnist dataset '''\n",
        "    #---------------- train data -------------------\n",
        "    mnist_train = datasets.MNIST('data', train=True, download=True)\n",
        "    data_train = mnist_train.data\n",
        "    labels_train = [mnist_train[i][1] for i in range(len(data_train))]\n",
        "\n",
        "    pics = []\n",
        "    for pic in data_train:\n",
        "        pic = to_pil_image(pic)\n",
        "        if img_size != 28:\n",
        "            pic = resize(pic, img_size)     # Resize image if needed\n",
        "        pic = to_tensor(pic)                # Tensor conversion normalizes in [0,1]\n",
        "        pics.append(pic)\n",
        "    data_train = torch.stack(pics)\n",
        "\n",
        "    #------------------  test data -----------------------\n",
        "    mnist_test = datasets.MNIST('data', train=False, download=True)\n",
        "    data_test = mnist_test.data\n",
        "    labels_test = [mnist_test[i][1] for i in range(len(data_test))]\n",
        "\n",
        "    pics = []\n",
        "    for pic in data_test:\n",
        "        pic = to_pil_image(pic)\n",
        "        if img_size != 28:\n",
        "            pic = resize(pic, img_size)     # Resize image if needed\n",
        "        pic = to_tensor(pic)                # Tensor conversion normalizes in [0,1]\n",
        "        pics.append(pic)\n",
        "    data_test = torch.stack(pics)\n",
        "\n",
        "    return (data_train,labels_train), (data_test,labels_test)\n",
        "\n",
        "def query(datafull, nb, bias=0, fav=0):\n",
        "    ''' returns -nb random samples of -datafull '''\n",
        "    data, labels = datafull\n",
        "    idxs = list(range(len(data)))\n",
        "    l = []\n",
        "    h, w = data[0][0].shape\n",
        "    d = torch.empty(nb, 1, h, w)\n",
        "    if bias == 0:\n",
        "        indexes = random.sample(idxs, nb) # drawing nb random indexes\n",
        "    else :\n",
        "        indexes = []\n",
        "        for i in range(nb):\n",
        "            idx = one_query(labels, idxs, bias, fav)\n",
        "            indexes.append(idx)\n",
        "            idxs.remove(idx) # to draw only once each index max\n",
        "    for k, i in enumerate(indexes): # filling our query\n",
        "        d[k] = data[i]\n",
        "        l.append(labels[i])\n",
        "    return d, l\n",
        "\n",
        "def one_query(labels, idxs, redraws, fav):\n",
        "    ''' labels : list of labels\n",
        "        idxs : list of available indexes\n",
        "        draws an index with a favorite label choice \n",
        "        fav : favorite label\n",
        "        redraws : max nb of random redraws while fav not found\n",
        "    '''\n",
        "    lab = -1 \n",
        "    while lab != fav and redraws >= 0:\n",
        "        idx = idxs[random.randint(0, len(idxs)-1)]\n",
        "        lab = labels[idx]\n",
        "        redraws -= 1\n",
        "    return idx\n",
        "\n",
        "def list_to_longtens(l):\n",
        "    ''' changes a list into torch.long tensor '''\n",
        "    tens = torch.empty(len(l), dtype=torch.long)  \n",
        "    for i, lab in enumerate(l):                       \n",
        "        tens[i] = lab\n",
        "    return tens\n",
        "\n",
        "def swap(l, n, m):\n",
        "    ''' swaps n and m values in l list '''\n",
        "    return [m if (v==n) else n if (v==m) else v for v in l]\n",
        "\n",
        "\n",
        "def distribute_data_rd(datafull, distrib, fav_lab=(-1,-1), dish=False, dish_lab=0, gpu=True): \n",
        "    '''draws random data on N nodes following distrib\n",
        "        data, labels : raw data and labels\n",
        "        distrib : int list, list of nb of data points for each node\n",
        "        pref_lab : (prefered label, strength of preference (int))\n",
        "        dish : boolean, if nodes are dishonest \n",
        "        dish_lab : 0 to 4, labelisation method\n",
        "\n",
        "        returns : (list of batches of images, list of batches of labels)\n",
        "    '''\n",
        "    data, labels = datafull\n",
        "    N = len(distrib)\n",
        "    data_dist = []      # list of len N\n",
        "    labels_dist = []    # list of len N\n",
        "    fav, strength = fav_lab\n",
        "\n",
        "    for n, number in enumerate(distrib): #for each node\n",
        "        if fav == -1:  # if no preference\n",
        "            d, l = query(datafull, number)\n",
        "        else:\n",
        "            d, l = query(datafull, number, strength, fav)\n",
        "        if gpu:\n",
        "            data_dist.append(torch.FloatTensor(d).cuda())\n",
        "        else:\n",
        "            data_dist.append(torch.FloatTensor(d))\n",
        "        if dish:                # if dishonest node\n",
        "\n",
        "            # labels modification\n",
        "            if dish_lab == 0: # random\n",
        "                tens = torch.randint(10,(number,), dtype=torch.long)\n",
        "            elif dish_lab == 1: # zeros\n",
        "                tens = torch.zeros(number, dtype=torch.long)\n",
        "            elif dish_lab == 2: # swap 1-7\n",
        "                l = swap(l, 1, 7)\n",
        "                tens = list_to_longtens(l)\n",
        "            elif dish_lab == 3: # swap 2 random (maybe same)\n",
        "                n, m = random.randint(0,10), random.randint(0,10)\n",
        "                l = swap(l, n, m)\n",
        "                tens = list_to_longtens(l)\n",
        "            elif dish_lab == 4: # label +1\n",
        "                tens = (list_to_longtens(l) + 1) % 10\n",
        "\n",
        "        else:           # if honest node \n",
        "            tens = list_to_longtens(l) # needed for CrossEntropy later\n",
        "        if gpu:\n",
        "            tens = tens.cuda()\n",
        "        labels_dist.append(tens)\n",
        "\n",
        "    return data_dist, labels_dist "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_HR2r0oK-Am"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QclvVEyDa4G"
      },
      "source": [
        "#model structure\n",
        "\n",
        "# def get_classifier():\n",
        "#     ''' returns linear baseline classifier '''\n",
        "#     model = nn.Sequential( \n",
        "#         nn.Flatten(),\n",
        "#         nn.Linear(1024, 10),\n",
        "#         nn.Sigmoid()\n",
        "#         )\n",
        "#     return model\n",
        "\n",
        "# def get_classifier():\n",
        "#     ''' returns classifier '''\n",
        "#     chan = 16\n",
        "#     model = nn.Sequential( \n",
        "#         nn.Conv2d(1, chan, kernel_size=3, padding=0),\n",
        "#         nn.Flatten(),\n",
        "#         nn.Linear(900 * chan, 10),\n",
        "#         nn.Sigmoid()\n",
        "#         )\n",
        "#     return model\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    '''CNN Model'''\n",
        "    def __init__(self):\n",
        "        super(classifier, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16,\n",
        "                              kernel_size=3, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, \n",
        "                              kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()      \n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        # Fully connected 1\n",
        "        self.fc1 = nn.Linear(32 * 6 * 6, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.maxpool1(out)  \n",
        "        # Set 2\n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        #Flatten\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #Dense\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "def get_classifier():\n",
        "    return classifier()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0v-iNHLBr4"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-q9ec13VV5z"
      },
      "source": [
        "### losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PH-0T9rrdr8"
      },
      "source": [
        "#loss and scoring functions \n",
        "\n",
        "def local_loss(model_loc, x, y):  \n",
        "    ''' classification loss '''\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    predicted = model_loc(x)\n",
        "    local = loss(predicted,y)\n",
        "    return local\n",
        "\n",
        "def generalisation_loss(model_loc, model_glob, pow):  \n",
        "    ''' l1 distance between global and local parameter\n",
        "    will be mutliplied by w_n \n",
        "    '''\n",
        "    q, p = pow\n",
        "    l1_norm = sum(((theta-rho)**q).abs().sum() for theta, rho in \n",
        "                  zip(model_loc.parameters(), model_glob.parameters()))**p\n",
        "    return l1_norm\n",
        "\n",
        "def regularisation_loss(model_glob, pow): \n",
        "    ''' l2 squared regularisation of global parameter\n",
        "     will be multiplied by w_0 \n",
        "     '''\n",
        "    q, p = pow\n",
        "    l2_sqr = sum((param**q).abs().sum() for param in model_glob.parameters())**p\n",
        "    return l2_sqr\n",
        "\n",
        "def round_loss(tens): \n",
        "    '''from an input scalar tensor returns rounded integer'''\n",
        "    if type(tens)==int or type(tens)==float:\n",
        "        return int(tens)\n",
        "    else:\n",
        "        return int(tens.item())\n",
        "\n",
        "def tens_count(tens, val):\n",
        "    ''' counts nb of -val in tensor -tens '''\n",
        "    return len(tens) - round_loss(torch.count_nonzero(tens-val))\n",
        "\n",
        "def score(model, datafull):\n",
        "    ''' returns accuracy provided models, images and GTs '''\n",
        "    out = model(datafull[0])\n",
        "    predictions = torch.max(out, 1)[1]\n",
        "    c=0\n",
        "    for a, b in zip(predictions, datafull[1]):\n",
        "        c += int(a==b)\n",
        "    return c/len(datafull[0])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1BWA7Kk99I"
      },
      "source": [
        "### Flower"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RNqTEQ9k8OQ"
      },
      "source": [
        "#nodes repartition\n",
        "\n",
        "class Flower():\n",
        "    ''' Training structure including local models and general one \n",
        "        Computing only the part of loss used at each epoch\n",
        "        Allowing to add and remove nodes at will\n",
        "        .pop\n",
        "        .add_nodes\n",
        "        .rem_nodes\n",
        "        .train\n",
        "        .display\n",
        "        .check\n",
        "    '''\n",
        "\n",
        "    def __init__(self, datafull_test, w_0, gpu=True):\n",
        "        ''' opt : optimizer\n",
        "            datafull_test : test data couple (imgs,labels)\n",
        "            w_0 : regularisation strength\n",
        "        '''\n",
        "        \n",
        "        self.d_test = datafull_test\n",
        "        self.w = w_0\n",
        "        self.gpu = gpu\n",
        "\n",
        "        self.opt = optim.Adam\n",
        "        self.lr_node = 0.001\n",
        "        self.lr_gen = 0.001\n",
        "        self.gen_freq = 1  # generalisation frequency (>0) \n",
        "        if self.gpu:\n",
        "            self.general_model = get_classifier().cuda() \n",
        "        else:\n",
        "            self.general_model = get_classifier() \n",
        "        self.opt_gen = self.opt(self.general_model.parameters(), lr=self.lr_gen)\n",
        "        self.pow_gen = (1,1)  # choice of norms for Licchavi loss \n",
        "        self.pow_reg = (2,1)  # (internal power, external power)\n",
        "        self.data = []\n",
        "        self.labels = [] \n",
        "        self.typ = []\n",
        "        self.models = []\n",
        "        self.W = []\n",
        "        self.age = []\n",
        "        self.opt_nodes = []\n",
        "        self.nb_nodes = 0\n",
        "        self.dic = {\"honest\" : -1, \"trolls\" : 0, \"zeros\" : 1, \n",
        "                    \"one_evil\" : 2, \"strats\" : 3, \"jokers\" : 4, \"byzantine\" : -1}\n",
        "        self.history = ([], [], [], []) # fit, gen, reg, acc\n",
        "        self.localtest = ([], []) # (which to pick for each node, list of (data,labels) pairs)\n",
        "\n",
        "    # ------------ population methods --------------------\n",
        "    def set_localtest(self, datafull, size, fav_lab, nodes):\n",
        "        ''' create a local data for some nodes\n",
        "            datafull : source data\n",
        "            size : size of test sample\n",
        "            fav_labs : (label, strength)\n",
        "            nodes : list of nodes which use this data           \n",
        "        '''\n",
        "        dt, lb = distribute_data_rd(datafull, [size], fav_lab,\n",
        "                                    dish=False, dish_lab=0, gpu=self.gpu)\n",
        "        dtloc = (dt[0], lb[0])\n",
        "        self.localtest[1].append(dtloc)\n",
        "        id = len(self.localtest[1]) - 1\n",
        "        for n in nodes:\n",
        "            self.localtest[0][n] = id\n",
        "\n",
        "    def add_nodes(self, datafull, pop, typ, fav_lab=(-1,-1), w=0.01):\n",
        "        ''' adds nodes to the Flower \n",
        "            datafull : data to put on node (sampled from it)\n",
        "            pop : (nb of nodes, size of nodes)\n",
        "            typ : type of nodes (str keywords)\n",
        "            fav_lab : (favorite label, strength)\n",
        "            w : int, weight of new nodes\n",
        "        '''\n",
        "        nb, size = pop\n",
        "        id = self.dic[typ]\n",
        "        dish = (id != -1) # boolean for dishonesty\n",
        "        dt, lb = distribute_data_rd(datafull, [size] * nb, fav_lab,\n",
        "                                    dish, dish_lab=id, gpu=self.gpu)\n",
        "        self.data += dt\n",
        "        self.labels += lb\n",
        "        self.typ += [typ] * nb\n",
        "        if self.gpu:\n",
        "            self.models += [get_classifier().cuda() for i in range(nb)]\n",
        "        else:\n",
        "            self.models += [get_classifier() for i in range(nb)]\n",
        "        self.W += [w] * nb\n",
        "        self.age += [0] * nb\n",
        "        for i in range(nb):\n",
        "            self.localtest[0].append(-1)\n",
        "        self.nb_nodes += nb\n",
        "        self.opt_nodes += [self.opt(self.models[n].parameters(), lr=self.lr_node) \n",
        "            for n in range(self.nb_nodes - nb, self.nb_nodes) \n",
        "            ]\n",
        "        print(\"Added {} {} nodes of {} data points\".format(nb, typ, size))\n",
        "        print(\"Total number of nodes : {}\".format(self.nb_nodes))\n",
        "\n",
        "    def rem_nodes(self, first, last):\n",
        "        ''' removes nodes of indexes -first (included) to -last (excluded) '''\n",
        "        nb = last - first\n",
        "        if last > self.nb_nodes:\n",
        "            print(\"-last is out of range, remove canceled\")\n",
        "        else:\n",
        "            del self.data[first : last]\n",
        "            del self.labels[first : last] \n",
        "            del self.typ[first : last]\n",
        "            del self.models[first : last]\n",
        "            del self.W[first : last]\n",
        "            del self.age[first : last]\n",
        "            del self.opt_nodes[first : last]\n",
        "            del self.localtest[0][first : last]\n",
        "            self.nb_nodes -= nb\n",
        "            print(\"Removed {} nodes\".format(nb))\n",
        "\n",
        "    def old(self, years):\n",
        "        ''' increments age (after training) '''\n",
        "        for i in range(self.nb_nodes):\n",
        "            self.age[i] += years\n",
        "        \n",
        "    def hm(self, ty):\n",
        "        ''' counts nb of nodes of this type '''\n",
        "        return self.typ.count(ty)\n",
        "    \n",
        "    def pop(self):\n",
        "        ''' returns dictionnary of population '''\n",
        "        c = {}\n",
        "        for ty in self.dic.keys():\n",
        "            c[ty] = self.hm(ty)\n",
        "        return c\n",
        "\n",
        "    # ------------- scoring methods -----------\n",
        "    def score_glob(self, datafull): \n",
        "        ''' returns accuracy provided images and GTs '''\n",
        "        return score(self.general_model, datafull)\n",
        "    \n",
        "    def test_loc(self, node):\n",
        "        ''' score of node on local test data '''\n",
        "        id_data = self.localtest[0][node]\n",
        "        if id_data == -1:\n",
        "            # print(\"No local test data\")\n",
        "            return None\n",
        "        else:\n",
        "            nodetest = score(self.models[node], self.localtest[1][id_data])\n",
        "            return nodetest\n",
        "\n",
        "    def test_full(self, node):\n",
        "        ''' score of node on global test data '''\n",
        "        return score(self.models[node], self.d_test)\n",
        "\n",
        "    def test_train(self, node):\n",
        "        ''' score of node on its train data '''\n",
        "        return score(self.models[node], (self.data[node], self.labels[node]))\n",
        "\n",
        "    def display(self, node):\n",
        "        ''' displays accuracy for selected node\n",
        "            node = -1 for global model\n",
        "        '''\n",
        "        if node == -1: # global model\n",
        "            print(\"global model\")\n",
        "            print(\"accuracy on test data :\", \n",
        "                  self.score_glob(self.d_test))\n",
        "        else: # we asked for a node\n",
        "            loc_train = self.test_train(node)\n",
        "            loc_test = self.test_loc(node)\n",
        "            full_test = self.test_full(node)\n",
        "            print(\"node number :\", node, \", dataset size :\",\n",
        "                len(self.labels[node]), \", type :\", self.typ[node], \n",
        "                \", age :\", self.age[node])\n",
        "            print(\"accuracy on local train data :\", loc_train)\n",
        "            print(\"accuracy on local test data :\", loc_test)\n",
        "            print(\"accuracy on global test data :\", full_test)\n",
        "            repart = {str(k) : tens_count(self.labels[node], k) \n",
        "                for k in range(10)}\n",
        "            print(\"labels repartition :\", repart)\n",
        "\n",
        "    # --------------------   TRAINING -------------------      \n",
        "    def train(self, nb_epochs=10, test_freq=1):   \n",
        "        '''training loop'''\n",
        "        time_train = time()\n",
        "\n",
        "        for n in range(self.nb_nodes):  # updating lr in optimizers\n",
        "            self.opt_nodes[n].param_groups[0]['lr'] = self.lr_node\n",
        "        self.opt_gen.param_groups[0]['lr'] = self.lr_gen\n",
        "\n",
        "        # initialisation to avoid undefined variables\n",
        "        loss = 0\n",
        "        fitting_loss = 0\n",
        "        harmonisation_loss = 0 \n",
        "        regul_loss = 0\n",
        "        count_fit, count_gen = 0, 0\n",
        "        \n",
        "        for epoch in range(1, nb_epochs+1):\n",
        "            time_ep1 = time()\n",
        "\n",
        "            # fitting or general epoch?\n",
        "            fit_epoch = (count_fit >= count_gen) \n",
        "            if fit_epoch:\n",
        "                count_gen += self.gen_freq\n",
        "            else:\n",
        "                count_fit += 1              \n",
        "            print(\"epoch :\",epoch,'/',nb_epochs, \n",
        "                  \"(fit)\" if fit_epoch else \"(gen)\")\n",
        "\n",
        "            #   only one step because we use all data at every epoch\n",
        "            # for step in range(1): \n",
        "            for n in range(self.nb_nodes):\n",
        "                self.opt_nodes[n].zero_grad()      \n",
        "            self.opt_gen.zero_grad()\n",
        "\n",
        "\n",
        "            #----------------    Licchavi loss  -------------------------\n",
        "            if fit_epoch: # only first 2 terms of loss updated\n",
        "                fitting_loss = 0\n",
        "                harmonisation_loss = 0 \n",
        "                diff = 0  # used to print correct loss\n",
        "                for n in range(self.nb_nodes):   # for each node\n",
        "                    if self.typ[n] == \"byzantine\":\n",
        "                        fit = local_loss(self.models[n], \n",
        "                                               self.data[n], self.labels[n])\n",
        "                        fitting_loss -= fit\n",
        "                        diff += 2 * fit\n",
        "                    else:\n",
        "                        fitting_loss += local_loss(self.models[n], \n",
        "                                               self.data[n], self.labels[n])\n",
        "                    g = generalisation_loss(self.models[n], \n",
        "                                            self.general_model, self.pow_gen)\n",
        "                    harmonisation_loss +=  self.W[n] * g  # generalisation term   \n",
        "                loss = fitting_loss + harmonisation_loss         \n",
        "\n",
        "            else: # only last 2 terms of loss updated        \n",
        "                harmonisation_loss = 0 \n",
        "                regul_loss = 0\n",
        "                for n in range(self.nb_nodes):   # for each node\n",
        "                    g = generalisation_loss(self.models[n], \n",
        "                                            self.general_model, self.pow_gen)\n",
        "                    harmonisation_loss += self.W[n] * g  #generalisation term    \n",
        "                r = regularisation_loss(self.general_model, self.pow_reg) \n",
        "                regul_loss = self.w * r   # regul term  \n",
        "                loss = harmonisation_loss + regul_loss\n",
        "\n",
        "            total_out = round_loss(fitting_loss + diff\n",
        "                  + harmonisation_loss + regul_loss)\n",
        "            print(\"total loss : \", total_out)  # printing losses\n",
        "            print(\"fitting : \", round_loss(fitting_loss + diff),\n",
        "                  ', harmonisation : ', round_loss(harmonisation_loss),\n",
        "                  ', regularisation : ', round_loss(regul_loss))\n",
        "            \n",
        "            self.history[0].append(round_loss(fitting_loss))\n",
        "            self.history[1].append(round_loss(harmonisation_loss))\n",
        "            self.history[2].append(round_loss(regul_loss)) \n",
        "\n",
        "            # ---------------------   Gradient descent ---------------------\n",
        "            loss.backward()      # backpropagation\n",
        "            if fit_epoch:       # updating local or global alternatively\n",
        "                for n in range(self.nb_nodes): \n",
        "                    self.opt_nodes[n].step()      # updating nodes weights \n",
        "            else:\n",
        "                self.opt_gen.step()       # updating general weights\n",
        "\n",
        "            time_ep2 = time()\n",
        "            print(\"epoch time :\", round(time_ep2 - time_ep1, 2))\n",
        "\n",
        "            if epoch % test_freq == 0:   # printing accuracy on test data\n",
        "                acc = self.score_glob(self.d_test)\n",
        "                print(\"\\nTEST ACCURACY : \", acc)\n",
        "                for i in range(test_freq):\n",
        "                    self.history[3].append(acc) \n",
        "            \n",
        "            self.old(1)  # aging all nodes\n",
        "            if False and not fit_epoch: # TEMPORARY adds nodes between epochs\n",
        "                roll = 100\n",
        "                self.add_nodes(train, (roll, 100), \"honest\", w=0.1)\n",
        "                self.rem_nodes(0, 0 + roll)\n",
        "            \n",
        "            print(\"\\n\")      # epoch end\n",
        "\n",
        "        # ----------------- end of training -------------------------------    \n",
        "        \n",
        "        for i in range(nb_epochs % test_freq): # to maintain same history length\n",
        "            self.history[3].append(acc)\n",
        "        print(\"training time :\", round(time() - time_train, 2)) \n",
        "    \n",
        "    def check(self):\n",
        "        ''' performs some tests on internal parameters adequation '''\n",
        "        b1 =  (self.nb_nodes == len(self.data) == len(self.labels) \n",
        "            == len(self.typ) == len(self.models) == len(self.opt_nodes) \n",
        "            == len(self.W) == len(self.age) == len(self.localtest[0]))\n",
        "        b2 = (len(self.history[0]) == len(self.history[1]) \n",
        "            == len(self.history[2]) == len(self.history[3]) >= max(self.age))\n",
        "        b3 = (max(self.localtest[0]) <= len(self.localtest[1]) - 1)\n",
        "        if (b1 and b2 and b3):\n",
        "            print(\"No Problem\")\n",
        "        else:\n",
        "            print(\"OULALA non ça va pas là\")\n",
        "        "
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHCoX9RGMtoI"
      },
      "source": [
        "## get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4inHbdwNgz4"
      },
      "source": [
        "# downloading data\n",
        "if 'train' not in globals(): # to avoid loading data every time\n",
        "    train, test = load_mnist()\n",
        "    test_gpu = torch.tensor(test[0]).cuda(), torch.tensor(test[1]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNiOich9viuc"
      },
      "source": [
        "## getting plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_uPE0aIC0Ai"
      },
      "source": [
        "# functions to display training history \n",
        "def show_loss(flow, title=None):\n",
        "    ''' plots curves of loss evolution of a Flower '''\n",
        "    fit, gen, reg, _ = flow.history\n",
        "    tot = [a + b + c for a, b, c in zip(fit, gen, reg)]\n",
        "    epochs = range(1,len(fit))\n",
        "    plt.plot(epochs, fit[1:],label='fit')\n",
        "    plt.plot(epochs, gen[1:],label='gen')\n",
        "    plt.plot(epochs, reg[1:],label='reg')      #outputs results as a plot\n",
        "    plt.plot(epochs, tot[1:],label='tot')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def show_acc(flow, title=None):\n",
        "    '''' plots curves of accuracy evolution of a Flower '''\n",
        "    acc = flow.history[3]\n",
        "    epochs = range(1,len(acc))\n",
        "    plt.plot(epochs, acc[1:],label='acc')\n",
        "    plt.ylim([0,1])\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkv4sxEP3X0j"
      },
      "source": [
        "# functions to train and display history at the end\n",
        "def run_disp(nbn, gpu=True):\n",
        "    ''' creates a flower of honest nodes and trains it for 200 eps\n",
        "        displays graphs of loss and accuracy \n",
        "        nbn : number of nodes\n",
        "    '''\n",
        "    w = 0.05 * nbn\n",
        "    if gpu:\n",
        "        flow = Flower(test_gpu, w, gpu=gpu)\n",
        "    else:\n",
        "        flow = Flower(test, w, gpu=gpu)\n",
        "    ppn = 60_000 // nbn # points per node\n",
        "    flow.add_nodes(train, (nbn, ppn), \"honest\", w=0.005)\n",
        "\n",
        "    epochs = 200\n",
        "    flow.lr_node = 0.001\n",
        "    flow.lr_gen = 0.01\n",
        "    flow.gen_freq = 1\n",
        "    flow.train(epochs, test_freq=2)\n",
        "    flow.check()\n",
        "    text = \"nbn : {}, ppn : {}, lrnode : {}, lrgen : {}, genfrq : {}, eps :{}\"\n",
        "    title = text.format(nbn, ppn, flow.lr_node, \n",
        "                        flow.lr_gen, flow.gen_freq, epochs)\n",
        "    show_acc(flow, title)\n",
        "    show_loss(flow, title)    \n",
        "    return flow\n",
        "\n",
        "def run_fracbyz(frac, typ, gpu=True):\n",
        "    ''' creates a flower of honest nodes and trains it for 200 eps\n",
        "        displays graphs of loss and accuracy \n",
        "        frac : fraction of dishonest nodes\n",
        "    '''\n",
        "    nbn = 1000\n",
        "    w = 0.05 * nbn\n",
        "    if gpu:\n",
        "        flow = Flower(test_gpu, w, gpu=gpu)\n",
        "    else:\n",
        "        flow = Flower(test, w, gpu=gpu)\n",
        "    ppn = 60_000 // nbn # points per node\n",
        "    nbbyz = int(frac * nbn)\n",
        "    nbh = nbn - nbbyz\n",
        "    \n",
        "    flow.add_nodes(train, (nbh, ppn), \"honest\", w=0.005)\n",
        "    flow.add_nodes(train, (nbbyz, ppn), typ, w=0.005)\n",
        "\n",
        "\n",
        "    epochs = 200\n",
        "    flow.lr_node = 0.001\n",
        "    flow.lr_gen = 0.01\n",
        "    flow.gen_freq = 1\n",
        "    flow.train(epochs, test_freq=2)\n",
        "    flow.check()\n",
        "    text = \"frac : {}, nbn : {}, lrnode : {}, lrgen : {}, genfrq : {}, eps :{}\"\n",
        "    title = text.format(frac, nbn, flow.lr_node, \n",
        "                        flow.lr_gen, flow.gen_freq, epochs)\n",
        "    show_acc(flow, title)\n",
        "    show_loss(flow, title)    \n",
        "    return flow\n",
        "\n",
        "def run_heter(heter, frac=0, typ=\"zeros\", gpu=True):\n",
        "    ''' creates a flower of honest nodes and trains it for 200 eps\n",
        "        displays graphs of loss and accuracy \n",
        "        frac : fraction of dishonest nodes\n",
        "        heter : heterogeneity of data\n",
        "    '''\n",
        "    nbn = 1000\n",
        "    w = 0.05 * nbn\n",
        "    if gpu:\n",
        "        flow = Flower(test_gpu, w, gpu=gpu)\n",
        "    else:\n",
        "        flow = Flower(test, w, gpu=gpu)\n",
        "    ppn = 60_000 // nbn # points per node\n",
        "    nbbyz = int(frac * nbn)\n",
        "    nbh = nbn - nbbyz\n",
        "    nbh_lab = nbh // 10 # for each label\n",
        "    nbbyz_lab = nbbyz // 10\n",
        "    \n",
        "    for lab in range(10):\n",
        "        flow.add_nodes(train, (nbh_lab, ppn), \"honest\", (lab, heter), w=0.005)\n",
        "        flow.add_nodes(train, (nbbyz_lab, ppn), typ, (lab, heter), w=0.005)\n",
        "\n",
        "    epochs = 200\n",
        "    flow.lr_node = 0.001\n",
        "    flow.lr_gen = 0.01\n",
        "    flow.gen_freq = 1\n",
        "    flow.train(epochs, test_freq=2)\n",
        "    flow.check()\n",
        "    t1 = \"heter : {}, nbn : {}, lrnode : {}, lrgen : {}, genfrq : {}, eps :{}\" \n",
        "    t2 = \"\\nfrac : {}, type : \" + typ \n",
        "    text = t1 + t2\n",
        "    title = text.format(heter, nbn, flow.lr_node, \n",
        "                        flow.lr_gen, flow.gen_freq, epochs, frac)\n",
        "    show_acc(flow, title)\n",
        "    show_loss(flow, title)    \n",
        "    return flow"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2MJp9My6fqI",
        "outputId": "ee22fbf2-bc0b-4742-f1bf-217ec2f74c49"
      },
      "source": [
        "flowheter = run_heter(20, frac=0.2, typ=\"byzantine\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 80\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 100\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 180\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 200\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 280\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 300\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 380\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 400\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 480\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 500\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 580\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 600\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 680\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 700\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 780\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 800\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 880\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 900\n",
            "Added 80 honest nodes of 60 data points\n",
            "Total number of nodes : 980\n",
            "Added 20 byzantine nodes of 60 data points\n",
            "Total number of nodes : 1000\n",
            "epoch : 1 / 200 (fit)\n",
            "total loss :  4913\n",
            "fitting :  2304 , harmonisation :  2609 , regularisation :  0\n",
            "epoch time : 3.94\n",
            "\n",
            "\n",
            "epoch : 2 / 200 (gen)\n",
            "total loss :  5894\n",
            "fitting :  2304 , harmonisation :  2565 , regularisation :  1025\n",
            "epoch time : 1.9\n",
            "\n",
            "TEST ACCURACY :  0.1057\n",
            "\n",
            "\n",
            "epoch : 3 / 200 (fit)\n",
            "total loss :  5504\n",
            "fitting :  2201 , harmonisation :  2278 , regularisation :  1025\n",
            "epoch time : 4.03\n",
            "\n",
            "\n",
            "epoch : 4 / 200 (gen)\n",
            "total loss :  5155\n",
            "fitting :  2201 , harmonisation :  2240 , regularisation :  713\n",
            "epoch time : 1.95\n",
            "\n",
            "TEST ACCURACY :  0.1365\n",
            "\n",
            "\n",
            "epoch : 5 / 200 (fit)\n",
            "total loss :  4914\n",
            "fitting :  2099 , harmonisation :  2101 , regularisation :  713\n",
            "epoch time : 3.96\n",
            "\n",
            "\n",
            "epoch : 6 / 200 (gen)\n",
            "total loss :  4683\n",
            "fitting :  2099 , harmonisation :  2068 , regularisation :  515\n",
            "epoch time : 1.94\n",
            "\n",
            "TEST ACCURACY :  0.1041\n",
            "\n",
            "\n",
            "epoch : 7 / 200 (fit)\n",
            "total loss :  4539\n",
            "fitting :  1994 , harmonisation :  2030 , regularisation :  515\n",
            "epoch time : 4.04\n",
            "\n",
            "\n",
            "epoch : 8 / 200 (gen)\n",
            "total loss :  4396\n",
            "fitting :  1994 , harmonisation :  2001 , regularisation :  400\n",
            "epoch time : 1.97\n",
            "\n",
            "TEST ACCURACY :  0.1037\n",
            "\n",
            "\n",
            "epoch : 9 / 200 (fit)\n",
            "total loss :  4282\n",
            "fitting :  1885 , harmonisation :  1996 , regularisation :  400\n",
            "epoch time : 4.01\n",
            "\n",
            "\n",
            "epoch : 10 / 200 (gen)\n",
            "total loss :  4184\n",
            "fitting :  1885 , harmonisation :  1969 , regularisation :  328\n",
            "epoch time : 1.95\n",
            "\n",
            "TEST ACCURACY :  0.1028\n",
            "\n",
            "\n",
            "epoch : 11 / 200 (fit)\n",
            "total loss :  4065\n",
            "fitting :  1780 , harmonisation :  1956 , regularisation :  328\n",
            "epoch time : 4.04\n",
            "\n",
            "\n",
            "epoch : 12 / 200 (gen)\n",
            "total loss :  3985\n",
            "fitting :  1780 , harmonisation :  1931 , regularisation :  273\n",
            "epoch time : 1.95\n",
            "\n",
            "TEST ACCURACY :  0.1026\n",
            "\n",
            "\n",
            "epoch : 13 / 200 (fit)\n",
            "total loss :  3872\n",
            "fitting :  1689 , harmonisation :  1909 , regularisation :  273\n",
            "epoch time : 4.09\n",
            "\n",
            "\n",
            "epoch : 14 / 200 (gen)\n",
            "total loss :  3803\n",
            "fitting :  1689 , harmonisation :  1884 , regularisation :  229\n",
            "epoch time : 1.99\n",
            "\n",
            "TEST ACCURACY :  0.1015\n",
            "\n",
            "\n",
            "epoch : 15 / 200 (fit)\n",
            "total loss :  3723\n",
            "fitting :  1628 , harmonisation :  1865 , regularisation :  229\n",
            "epoch time : 4.01\n",
            "\n",
            "\n",
            "epoch : 16 / 200 (gen)\n",
            "total loss :  3667\n",
            "fitting :  1628 , harmonisation :  1841 , regularisation :  196\n",
            "epoch time : 1.92\n",
            "\n",
            "TEST ACCURACY :  0.16\n",
            "\n",
            "\n",
            "epoch : 17 / 200 (fit)\n",
            "total loss :  3635\n",
            "fitting :  1612 , harmonisation :  1826 , regularisation :  196\n",
            "epoch time : 4.02\n",
            "\n",
            "\n",
            "epoch : 18 / 200 (gen)\n",
            "total loss :  3589\n",
            "fitting :  1612 , harmonisation :  1803 , regularisation :  174\n",
            "epoch time : 1.96\n",
            "\n",
            "TEST ACCURACY :  0.1381\n",
            "\n",
            "\n",
            "epoch : 19 / 200 (fit)\n",
            "total loss :  3614\n",
            "fitting :  1647 , harmonisation :  1792 , regularisation :  174\n",
            "epoch time : 4.06\n",
            "\n",
            "\n",
            "epoch : 20 / 200 (gen)\n",
            "total loss :  3576\n",
            "fitting :  1647 , harmonisation :  1769 , regularisation :  158\n",
            "epoch time : 1.91\n",
            "\n",
            "TEST ACCURACY :  0.1306\n",
            "\n",
            "\n",
            "epoch : 21 / 200 (fit)\n",
            "total loss :  3663\n",
            "fitting :  1735 , harmonisation :  1768 , regularisation :  158\n",
            "epoch time : 4.02\n",
            "\n",
            "\n",
            "epoch : 22 / 200 (gen)\n",
            "total loss :  3631\n",
            "fitting :  1735 , harmonisation :  1745 , regularisation :  149\n",
            "epoch time : 1.94\n",
            "\n",
            "TEST ACCURACY :  0.0981\n",
            "\n",
            "\n",
            "epoch : 23 / 200 (fit)\n",
            "total loss :  3771\n",
            "fitting :  1871 , harmonisation :  1751 , regularisation :  149\n",
            "epoch time : 4.02\n",
            "\n",
            "\n",
            "epoch : 24 / 200 (gen)\n",
            "total loss :  3743\n",
            "fitting :  1871 , harmonisation :  1728 , regularisation :  143\n",
            "epoch time : 1.96\n",
            "\n",
            "TEST ACCURACY :  0.098\n",
            "\n",
            "\n",
            "epoch : 25 / 200 (fit)\n",
            "total loss :  3922\n",
            "fitting :  2045 , harmonisation :  1733 , regularisation :  143\n",
            "epoch time : 4.04\n",
            "\n",
            "\n",
            "epoch : 26 / 200 (gen)\n",
            "total loss :  3893\n",
            "fitting :  2045 , harmonisation :  1710 , regularisation :  137\n",
            "epoch time : 1.95\n",
            "\n",
            "TEST ACCURACY :  0.098\n",
            "\n",
            "\n",
            "epoch : 27 / 200 (fit)\n",
            "total loss :  4100\n",
            "fitting :  2252 , harmonisation :  1710 , regularisation :  137\n",
            "epoch time : 4.0\n",
            "\n",
            "\n",
            "epoch : 28 / 200 (gen)\n",
            "total loss :  4068\n",
            "fitting :  2252 , harmonisation :  1687 , regularisation :  128\n",
            "epoch time : 1.92\n",
            "\n",
            "TEST ACCURACY :  0.0992\n",
            "\n",
            "\n",
            "epoch : 29 / 200 (fit)\n",
            "total loss :  4298\n",
            "fitting :  2488 , harmonisation :  1682 , regularisation :  128\n",
            "epoch time : 4.05\n",
            "\n",
            "\n",
            "epoch : 30 / 200 (gen)\n",
            "total loss :  4263\n",
            "fitting :  2488 , harmonisation :  1659 , regularisation :  116\n",
            "epoch time : 1.94\n",
            "\n",
            "TEST ACCURACY :  0.1611\n",
            "\n",
            "\n",
            "epoch : 31 / 200 (fit)\n",
            "total loss :  4515\n",
            "fitting :  2749 , harmonisation :  1649 , regularisation :  116\n",
            "epoch time : 4.08\n",
            "\n",
            "\n",
            "epoch : 32 / 200 (gen)\n",
            "total loss :  4477\n",
            "fitting :  2749 , harmonisation :  1626 , regularisation :  102\n",
            "epoch time : 1.94\n",
            "\n",
            "TEST ACCURACY :  0.1016\n",
            "\n",
            "\n",
            "epoch : 33 / 200 (fit)\n",
            "total loss :  4753\n",
            "fitting :  3036 , harmonisation :  1615 , regularisation :  102\n",
            "epoch time : 4.08\n",
            "\n",
            "\n",
            "epoch : 34 / 200 (gen)\n",
            "total loss :  4715\n",
            "fitting :  3036 , harmonisation :  1591 , regularisation :  87\n",
            "epoch time : 1.95\n",
            "\n",
            "TEST ACCURACY :  0.1141\n",
            "\n",
            "\n",
            "epoch : 35 / 200 (fit)\n",
            "total loss :  5019\n",
            "fitting :  3351 , harmonisation :  1580 , regularisation :  87\n",
            "epoch time : 3.98\n",
            "\n",
            "\n",
            "epoch : 36 / 200 (gen)\n",
            "total loss :  4982\n",
            "fitting :  3351 , harmonisation :  1557 , regularisation :  72\n",
            "epoch time : 1.93\n",
            "\n",
            "TEST ACCURACY :  0.0958\n",
            "\n",
            "\n",
            "epoch : 37 / 200 (fit)\n",
            "total loss :  5320\n",
            "fitting :  3698 , harmonisation :  1549 , regularisation :  72\n",
            "epoch time : 4.05\n",
            "\n",
            "\n",
            "epoch : 38 / 200 (gen)\n",
            "total loss :  5286\n",
            "fitting :  3698 , harmonisation :  1527 , regularisation :  60\n",
            "epoch time : 1.98\n",
            "\n",
            "TEST ACCURACY :  0.0958\n",
            "\n",
            "\n",
            "epoch : 39 / 200 (fit)\n",
            "total loss :  5662\n",
            "fitting :  4079 , harmonisation :  1522 , regularisation :  60\n",
            "epoch time : 4.05\n",
            "\n",
            "\n",
            "epoch : 40 / 200 (gen)\n",
            "total loss :  5631\n",
            "fitting :  4079 , harmonisation :  1502 , regularisation :  49\n",
            "epoch time : 1.96\n",
            "\n",
            "TEST ACCURACY :  0.1625\n",
            "\n",
            "\n",
            "epoch : 41 / 200 (fit)\n",
            "total loss :  6047\n",
            "fitting :  4499 , harmonisation :  1498 , regularisation :  49\n",
            "epoch time : 4.08\n",
            "\n",
            "\n",
            "epoch : 42 / 200 (gen)\n",
            "total loss :  6020\n",
            "fitting :  4499 , harmonisation :  1479 , regularisation :  41\n",
            "epoch time : 1.94\n",
            "\n",
            "TEST ACCURACY :  0.0977\n",
            "\n",
            "\n",
            "epoch : 43 / 200 (fit)\n",
            "total loss :  6479\n",
            "fitting :  4961 , harmonisation :  1476 , regularisation :  41\n",
            "epoch time : 4.04\n",
            "\n",
            "\n",
            "epoch : 44 / 200 (gen)\n",
            "total loss :  6454\n",
            "fitting :  4961 , harmonisation :  1458 , regularisation :  34\n",
            "epoch time : 1.97\n",
            "\n",
            "TEST ACCURACY :  0.0991\n",
            "\n",
            "\n",
            "epoch : 45 / 200 (fit)\n",
            "total loss :  6959\n",
            "fitting :  5471 , harmonisation :  1453 , regularisation :  34\n",
            "epoch time : 4.06\n",
            "\n",
            "\n",
            "epoch : 46 / 200 (gen)\n",
            "total loss :  6937\n",
            "fitting :  5471 , harmonisation :  1437 , regularisation :  28\n",
            "epoch time : 1.95\n",
            "\n",
            "TEST ACCURACY :  0.2843\n",
            "\n",
            "\n",
            "epoch : 47 / 200 (fit)\n",
            "total loss :  7492\n",
            "fitting :  6031 , harmonisation :  1432 , regularisation :  28\n",
            "epoch time : 3.98\n",
            "\n",
            "\n",
            "epoch : 48 / 200 (gen)\n",
            "total loss :  7473\n",
            "fitting :  6031 , harmonisation :  1416 , regularisation :  24\n",
            "epoch time : 1.93\n",
            "\n",
            "TEST ACCURACY :  0.1415\n",
            "\n",
            "\n",
            "epoch : 49 / 200 (fit)\n",
            "total loss :  8085\n",
            "fitting :  6646 , harmonisation :  1413 , regularisation :  24\n",
            "epoch time : 4.06\n",
            "\n",
            "\n",
            "epoch : 50 / 200 (gen)\n",
            "total loss :  8067\n",
            "fitting :  6646 , harmonisation :  1399 , regularisation :  21\n",
            "epoch time : 1.96\n",
            "\n",
            "TEST ACCURACY :  0.1032\n",
            "\n",
            "\n",
            "epoch : 51 / 200 (fit)\n",
            "total loss :  8740\n",
            "fitting :  7320 , harmonisation :  1398 , regularisation :  21\n",
            "epoch time : 4.08\n",
            "\n",
            "\n",
            "epoch : 52 / 200 (gen)\n",
            "total loss :  8726\n",
            "fitting :  7320 , harmonisation :  1385 , regularisation :  19\n",
            "epoch time : 1.92\n",
            "\n",
            "TEST ACCURACY :  0.1032\n",
            "\n",
            "\n",
            "epoch : 53 / 200 (fit)\n",
            "total loss :  9462\n",
            "fitting :  8056 , harmonisation :  1386 , regularisation :  19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VOWD2aPGT43"
      },
      "source": [
        "#manual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTX8hvSTTWIY"
      },
      "source": [
        "tulip = Flower(test_gpu, 0.01)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SIi5Z7Ukd78",
        "outputId": "0e26972d-20be-4e1f-9bfe-690dd8923291"
      },
      "source": [
        "tulip.add_nodes(train, (90, 600), \"honest\", w=0.005)\n",
        "tulip.add_nodes(train, (10, 600), \"honest\", (0,10), w=0.005)\n",
        "tulip.set_localtest(test, 1000, (-1,-1), range(0,50))\n",
        "tulip.set_localtest(test, 1000, (0,10), range(50,100))\n",
        "tulip.check()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Added 90 honest nodes of 600 data points\n",
            "Total number of nodes : 190\n",
            "Added 10 honest nodes of 600 data points\n",
            "Total number of nodes : 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xodSD2PM-eY-",
        "outputId": "38c8b8e6-546a-465e-898b-3404dc69482c"
      },
      "source": [
        "tulip.display(57)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node number : 57 , dataset size : 600 , type : honest , age : 0\n",
            "accuracy on local train data : 0.09166666666666666\n",
            "accuracy on local test data : 0.053\n",
            "accuracy on global test data : 0.0953\n",
            "labels repartition : {'0': 67, '1': 67, '2': 59, '3': 65, '4': 57, '5': 56, '6': 59, '7': 59, '8': 51, '9': 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5-ZxeKMvGz"
      },
      "source": [
        "tulip.w = 5\n",
        "tulip.lr_node = 0.001\n",
        "tulip.lr_gen = 0.01\n",
        "tulip.gen_freq = 1\n",
        "tulip.train(100, test_freq=10)\n",
        "tulip.check()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIv7X1MIW8Qw"
      },
      "source": [
        "###display "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "GXvbqG3DYHNg",
        "outputId": "c7f8d276-972b-450c-c85f-b750b61926d2"
      },
      "source": [
        "show_acc(tulip)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "oFc7OUe-bDIx",
        "outputId": "ce1161db-74b4-4db6-a578-1124fe2c0dcd"
      },
      "source": [
        "show_loss(tulip)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSRqfKLyWVx"
      },
      "source": [
        "# Brouillon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Ot3Gsk-W2A"
      },
      "source": [
        "def destroy(obj):   \n",
        "    del obj\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}